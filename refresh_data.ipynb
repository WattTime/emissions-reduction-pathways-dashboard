{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c52d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a script to take raw csvs in data/raw_csvs folder and covert them to \n",
    "parquets for manageable GitHub storage and limited memory usage (DuckDB).\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set input and output directories\n",
    "input_dir = Path(\"data/raw_csvs\")\n",
    "output_dir = Path(\"data\")\n",
    "\n",
    "# Make sure the output directory exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loop through all CSV files in the input directory\n",
    "for csv_file in input_dir.glob(\"*.csv\"):\n",
    "    print(f\"Converting {csv_file.name}...\")\n",
    "\n",
    "    # Read CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create output path by replacing .csv with .parquet\n",
    "    parquet_file = output_dir / csv_file.with_suffix(\".parquet\").name\n",
    "\n",
    "    # Write to Parquet\n",
    "    df.to_parquet(parquet_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Saved to {parquet_file}\")\n",
    "\n",
    "     # Delete original CSV\n",
    "    csv_file.unlink()\n",
    "    print(f\"Deleted original CSV: {csv_file.name}\")\n",
    "\n",
    "print(\"✅ CSV to Parquet conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347b58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting max month...\n",
      "Running asset-level query and writing to parquet file, this may take a while...\n",
      "✅ Asset parquet file exported\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code updates the asset data\n",
    "\n",
    "'''\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/asset_emissions_country_subsector.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "print(\"Getting max month...\")\n",
    "max_date = con.execute(f\"\"\"\n",
    "    select max(start_time)\n",
    "    from postgres_scan('{postgres_url}', 'public', 'asset_emissions')                       \n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(\"Running asset-level query and writing to parquet file, this may take a while...\")\n",
    "con.execute(f\"\"\"\n",
    "    INSTALL postgres;\n",
    "    LOAD postgres;\n",
    "\n",
    "    CREATE TABLE asset_emissions_parquet AS\n",
    "    SELECT ae.iso3_country,\n",
    "        ae.original_inventory_sector,\n",
    "        ae.start_time,\n",
    "        ae.gas,\n",
    "        sch.sector,\n",
    "        ca.name as country_name,\n",
    "        ca.continent,\n",
    "        ca.unfccc_annex,\n",
    "        ca.em_finance,\n",
    "        ca.eu,\n",
    "        ca.oecd,\n",
    "        ca.developed_un,\n",
    "        ae.release,\n",
    "        sum(emissions_quantity) emissions_quantity,\n",
    "        sum(activity) activity,\n",
    "        sum(emissions_quantity) / sum(activity) weighted_average_emissions_factor\n",
    "    \n",
    "    FROM postgres_scan('{postgres_url}', 'public', 'asset_emissions') ae\n",
    "    LEFT JOIN postgres_scan('{postgres_url}', 'public', 'country_analysis') ca\n",
    "        ON CAST(ca.iso3_country AS VARCHAR) = CAST(ae.iso3_country AS VARCHAR)\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT sector, subsector FROM postgres_scan('{postgres_url}', 'public', 'asset_schema')\n",
    "    ) sch\n",
    "        ON CAST(sch.subsector AS VARCHAR) = CAST(ae.original_inventory_sector AS VARCHAR)\n",
    "    \n",
    "    WHERE ae.start_time >= (\n",
    "                date_trunc('year', DATE '{max_date}') - INTERVAL '3 YEARS'\n",
    "            )\n",
    "      AND ae.gas in ('co2e_100yr','ch4')\n",
    "      AND ae.most_granular = TRUE\n",
    "    \n",
    "    GROUP BY ae.iso3_country,\n",
    "        ae.original_inventory_sector,\n",
    "        ae.start_time,\n",
    "        ae.gas,\n",
    "        sch.sector,\n",
    "        ca.name,\n",
    "        ca.continent,\n",
    "        ca.unfccc_annex,\n",
    "        ca.em_finance,\n",
    "        ca.eu,\n",
    "        ca.oecd,\n",
    "        ca.developed_un,\n",
    "        ae.release;\n",
    "\n",
    "    COPY asset_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "con.close()\n",
    "\n",
    "print(\"✅ Asset parquet file exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8f6c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ Asset Annual Emissions ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/emissions_reduction/asset_annual_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "print('Running query...')\n",
    "con.execute( f'''\n",
    "\tINSTALL postgres;\n",
    "\tLOAD postgres;\n",
    "\n",
    "\tCREATE TABLE asset_annual_emissions_parquet AS\n",
    "\tselect extract(year from ae.start_time) as year\n",
    "\t\t, ae.asset_id\n",
    "\t\t, ai.asset_type\n",
    "\t\t, CASE \n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'iron-and-steel' AND ai.asset_type LIKE '%BF%' \n",
    "\t\t\t\t\tTHEN '{{''iron-and-steel'': [''BF'', ''DRI-EAF'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Refinery%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Refinery'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Smelting%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Smelting'']}}'\n",
    "\t\t\t\tELSE 'all' \n",
    "\t\t\tEND AS asset_type_2\n",
    "\t\t, ai.asset_name\n",
    "\t\t, ae.iso3_country\n",
    "\t\t, ca.name as country_name\n",
    "        , abc.region balancing_authority_region\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ae.original_inventory_sector as subsector\n",
    "\t\t, al.gadm_1\n",
    "\t\t, al.gadm_2\n",
    "\t\t, al.ghs_fua\n",
    "\t\t, al.city_id\n",
    "\t\t, ae.other1\n",
    "\t\t, ae.other2\n",
    "\t\t, ae.other3\n",
    "\t\t, ae.other4\n",
    "\t\t, ae.other5\n",
    "\t\t, ae.other6\n",
    "\t\t, ae.other7\n",
    "\t\t, ae.other8\n",
    "\t\t, ae.other9\n",
    "\t\t, ae.other10\n",
    "\t\t, ae.activity_units\n",
    "\t\t, sum(capacity) capacity\n",
    "\t\t, sum(activity) activity\n",
    "\t\t, avg(emissions_factor) average_emissions_factor\n",
    "\t\t, sum(emissions_quantity) emissions_quantity\n",
    "        , ers.strategy_id\n",
    "\t\t, ers.strategy_name\n",
    "\t\t, ers.strategy_description\n",
    "\t\t, ers.mechanism\n",
    "\t\t, ers.old_activity\n",
    "\t\t, ers.affected_activity\n",
    "\t\t, ers.old_emissions_factor\n",
    "\t\t, ers.new_emissions_factor\n",
    "\t\t, ers.emissions_reduced_at_asset\n",
    "\t\t, ers.induced_sector_1\n",
    "\t\t, ers.induced_sector_1_induced_emissions\n",
    "\t\t, ers.induced_sector_2\n",
    "\t\t, ers.induced_sector_2_induced_emissions\n",
    "\t\t, ers.induced_sector_3\n",
    "\t\t, ers.induced_sector_3_induced_emissions\n",
    "\t\t, ers.total_emissions_reduced_per_year\n",
    "\n",
    "\tfrom postgres_scan('{postgres_url}','public', 'asset_emissions') ae\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'asset_information') ai\n",
    "\t\ton ai.asset_id = ae.asset_id\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'asset_location') al\n",
    "\t\ton al.asset_id = ae.asset_id\n",
    "\tleft join (\n",
    "\t\tselect distinct sector, subsector from postgres_scan('{postgres_url}','public', 'asset_schema')\n",
    "\t) asch\n",
    "\t\ton cast(asch.subsector as varchar) = cast(ae.original_inventory_sector as varchar)\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(ae.iso3_country as varchar)\n",
    "    left join postgres_scan('{postgres_url}','public', 'asset_ba_crosswalk') abc\n",
    "\t\ton abc.asset_id = ae.asset_id\n",
    "    left join (\n",
    "\t\tselect rdf.* \n",
    "\n",
    "\t\tfrom postgres_scan('{postgres_url}','public','reductions_data_fusion') rdf\n",
    "\t\tinner join (\n",
    "\t\t\tSELECT *\n",
    "\t\t\tFROM postgres_scan('{postgres_url}','public', 'strategy_crosswalk_staging')\n",
    "\t\t\tWHERE strategy_rank = 1\n",
    "\t\t\t\n",
    "\t\t\tUNION\n",
    "\t\t\t\n",
    "\t\t\tSELECT *\n",
    "\t\t\tFROM postgres_scan('{postgres_url}','public', 'strategy_crosswalk_staging') a\n",
    "\t\t\tWHERE strategy_rank IS NULL\n",
    "\t\t\tAND NOT EXISTS (\n",
    "\t\t\t\tSELECT 1\n",
    "\t\t\t\tFROM postgres_scan('{postgres_url}','public', 'strategy_crosswalk_staging') b\n",
    "\t\t\t\tWHERE a.asset_id = b.asset_id \n",
    "\t\t\t\t\tAND b.strategy_rank IS NOT NULL\n",
    "\t\t\t)\n",
    "\t\t) sc\n",
    "\t\t\ton sc.asset_id = rdf.asset_id\n",
    "\t\t\tand sc.strategy_id = rdf.strategy_id\n",
    "\t\t\tand rdf.gas = 'co2e_100yr'\n",
    "    ) ers\n",
    "\t\ton ers.asset_id = ae.asset_id\n",
    "\n",
    "\twhere extract(year from ae.start_time) = 2024\n",
    "\t\tand ae.most_granular = true\n",
    "\t\tand ae.gas = 'co2e_100yr'\n",
    "\t\tand ae.original_inventory_sector not in ('forest-land-clearing',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-degradation',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'net-forest-land',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'net-shrubgrass',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'net-wetland',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'removals',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'shrubgrass-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'water-reservoirs',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'wetland-fires')\n",
    "\n",
    "\tgroup by extract(year from ae.start_time)\n",
    "\t\t, ae.asset_id\n",
    "\t\t, ai.asset_type\n",
    "        , CASE \n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'iron-and-steel' AND ai.asset_type LIKE '%BF%' \n",
    "\t\t\t\t\tTHEN '{{''iron-and-steel'': [''BF'', ''DRI-EAF'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Refinery%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Refinery'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Smelting%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Smelting'']}}'\n",
    "\t\t\t\tELSE 'all' \n",
    "\t\t\tEND\n",
    "\t\t, ai.asset_name\n",
    "\t\t, ae.iso3_country\n",
    "\t\t, ca.name\n",
    "        , abc.region\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ae.original_inventory_sector\n",
    "\t\t, al.gadm_1\n",
    "\t\t, al.gadm_2\n",
    "\t\t, al.ghs_fua\n",
    "\t\t, al.city_id\n",
    "\t\t, ae.other1\n",
    "\t\t, ae.other2\n",
    "\t\t, ae.other3\n",
    "\t\t, ae.other4\n",
    "\t\t, ae.other5\n",
    "\t\t, ae.other6\n",
    "\t\t, ae.other7\n",
    "\t\t, ae.other8\n",
    "\t\t, ae.other9\n",
    "\t\t, ae.other10\n",
    "\t\t, ae.activity_units\n",
    "        , ers.strategy_id\n",
    "\t\t, ers.strategy_name\n",
    "\t\t, ers.strategy_description\n",
    "\t\t, ers.mechanism\n",
    "\t\t, ers.old_activity\n",
    "\t\t, ers.affected_activity\n",
    "\t\t, ers.old_emissions_factor\n",
    "\t\t, ers.new_emissions_factor\n",
    "\t\t, ers.emissions_reduced_at_asset\n",
    "\t\t, ers.induced_sector_1\n",
    "\t\t, ers.induced_sector_1_induced_emissions\n",
    "\t\t, ers.induced_sector_2\n",
    "\t\t, ers.induced_sector_2_induced_emissions\n",
    "\t\t, ers.induced_sector_3\n",
    "\t\t, ers.induced_sector_3_induced_emissions\n",
    "\t\t, ers.total_emissions_reduced_per_year;\n",
    "            \n",
    "    COPY asset_annual_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "            \n",
    "    ''')\n",
    "\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295e549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------- ADD MOER FACTORS --------------------------------------\n",
    "\n",
    "# import duckdb\n",
    "from utils.utils import data_add_moer\n",
    "import pandas as pd\n",
    "\n",
    "asset_parquet_path = 'data/emissions_reduction/asset_annual_emissions.parquet'\n",
    "output_path = 'data/emissions_reduction/asset_annual_emissions_moer.parquet'\n",
    "\n",
    "df_asset = pd.read_parquet(asset_parquet_path)\n",
    "\n",
    "asset_moer_df = data_add_moer(df_asset, cond={\"moer\": True})\n",
    "\n",
    "asset_moer_df.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e07028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/asset_annual_emissions/chunk_1.parquet (51.1 MB, rows 0–428377)\n",
      "Saved data/asset_annual_emissions/chunk_2.parquet (51.2 MB, rows 428377–856754)\n",
      "Saved data/asset_annual_emissions/chunk_3.parquet (51.3 MB, rows 856754–1285131)\n",
      "Saved data/asset_annual_emissions/chunk_4.parquet (51.2 MB, rows 1285131–1713508)\n",
      "Saved data/asset_annual_emissions/chunk_5.parquet (51.2 MB, rows 1713508–2141885)\n",
      "Saved data/asset_annual_emissions/chunk_6.parquet (44.8 MB, rows 2141885–2513420)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ SPLITS LARGE ASSET FILE INTO ~50MB CHUNKS ---------------------------------\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "input_file = \"data/emissions_reduction/asset_annual_emissions_moer.parquet\"  # Your large file\n",
    "output_dir = \"data/asset_annual_emissions\"  # Destination folder\n",
    "target_size_mb = 50  # Keep each file safely under 100MB\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load full Parquet into DataFrame\n",
    "df = pd.read_parquet(input_file)\n",
    "total_rows = len(df)\n",
    "\n",
    "# Estimate file size per row using a small sample\n",
    "test_sample = df.iloc[:10000]\n",
    "test_table = pa.Table.from_pandas(test_sample)\n",
    "pq.write_table(test_table, \"temp.parquet\")\n",
    "bytes_per_row = os.path.getsize(\"temp.parquet\") / len(test_sample)\n",
    "os.remove(\"temp.parquet\")\n",
    "\n",
    "# Determine number of rows per ~50MB chunk\n",
    "target_bytes = target_size_mb * 1024 * 1024\n",
    "rows_per_chunk = int(target_bytes / bytes_per_row)\n",
    "\n",
    "# Split and write files\n",
    "for i, start in enumerate(range(0, total_rows, rows_per_chunk)):\n",
    "    end = min(start + rows_per_chunk, total_rows)\n",
    "    chunk_df = df.iloc[start:end]\n",
    "    chunk_table = pa.Table.from_pandas(chunk_df)\n",
    "    output_path = os.path.join(output_dir, f\"chunk_{i+1}.parquet\")\n",
    "    pq.write_table(chunk_table, output_path)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"Saved {output_path} ({size_mb:.1f} MB, rows {start}–{end})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8defd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------ GADM 1 Emissions ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/emissions_reduction/gadm_1_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "print('Running query')\n",
    "con.execute(f'''\n",
    "    INSTALL postgres;\n",
    "    LOAD postgres;\n",
    "\n",
    "    CREATE TABLE gadm_1_emissions_parquet AS\n",
    "    select extract(year from g1e.start_time) as year \n",
    "        , g1e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g1e.iso3_country\n",
    "        , ca.name as country_name\n",
    "        , gb.name gadm_1_name\n",
    "        , gb.corrected_name gadm_1_corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g1e.original_inventory_sector subsector\n",
    "        , g1e.gas\n",
    "        , sum(asset_activity) asset_activity\n",
    "        , sum(asset_emissions) asset_emissions\n",
    "        , sum(remainder_activity) remainder_activity\n",
    "        , sum(remainder_emissions) remainder_emissions\n",
    "        , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "    from postgres_scan('{postgres_url}', 'public', 'gadm_1_emissions') g1e\n",
    "    inner join (\n",
    "        select distinct gadm_id\n",
    "            , gid\n",
    "            , name\n",
    "            , corrected_name\n",
    "            , admin_level\n",
    "        from postgres_scan('{postgres_url}','public', 'gadm_boundaries') \n",
    "        where admin_level = 1\n",
    "    ) as gb\n",
    "        on g1e.gadm_id = gb.gadm_id\n",
    "    left join (\n",
    "        select distinct sector\n",
    "            , subsector\n",
    "        from postgres_scan('{postgres_url}','public', 'asset_schema') \n",
    "    ) asch\n",
    "        on cast(asch.subsector as varchar) = cast(g1e.original_inventory_sector as varchar)\n",
    "    left join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(g1e.iso3_country as varchar)\n",
    "\n",
    "    where g1e.gas = 'co2e_100yr'\n",
    "        and extract(year from start_time) = 2024\n",
    "        and g1e.original_inventory_sector not in ('forest-land-clearing',\n",
    "                                                'forest-land-degradation',\n",
    "                                                'forest-land-fires',\n",
    "                                                'net-forest-land',\n",
    "                                                'net-shrubgrass',\n",
    "                                                'net-wetland',\n",
    "                                                'removals',\n",
    "                                                'shrubgrass-fires',\n",
    "                                                'water-reservoirs',\n",
    "                                                'wetland-fires')\n",
    "\n",
    "    group by extract(year from g1e.start_time) \n",
    "        , g1e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g1e.iso3_country\n",
    "        , ca.name\n",
    "        , gb.name \n",
    "        , gb.corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g1e.original_inventory_sector\n",
    "        , g1e.gas;\n",
    "\n",
    "    COPY gadm_1_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "''')\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04194ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing gadm_2 query...\n",
      "Processed batch 1 (10000 rows), total rows: 10000\n",
      "Processed batch 2 (10000 rows), total rows: 20000\n",
      "Processed batch 3 (10000 rows), total rows: 30000\n",
      "Processed batch 4 (10000 rows), total rows: 40000\n",
      "Processed batch 5 (10000 rows), total rows: 50000\n",
      "Processed batch 6 (10000 rows), total rows: 60000\n",
      "Processed batch 7 (10000 rows), total rows: 70000\n",
      "Processed batch 8 (10000 rows), total rows: 80000\n",
      "Processed batch 9 (10000 rows), total rows: 90000\n",
      "Processed batch 10 (10000 rows), total rows: 100000\n",
      "Processed batch 11 (10000 rows), total rows: 110000\n",
      "Processed batch 12 (10000 rows), total rows: 120000\n",
      "Processed batch 13 (10000 rows), total rows: 130000\n",
      "Processed batch 14 (10000 rows), total rows: 140000\n",
      "Processed batch 15 (10000 rows), total rows: 150000\n",
      "Processed batch 16 (10000 rows), total rows: 160000\n",
      "Processed batch 17 (10000 rows), total rows: 170000\n",
      "Processed batch 18 (10000 rows), total rows: 180000\n",
      "Processed batch 19 (10000 rows), total rows: 190000\n",
      "Processed batch 20 (10000 rows), total rows: 200000\n",
      "Processed batch 21 (10000 rows), total rows: 210000\n",
      "Processed batch 22 (10000 rows), total rows: 220000\n",
      "Processed batch 23 (10000 rows), total rows: 230000\n",
      "Processed batch 24 (10000 rows), total rows: 240000\n",
      "Processed batch 25 (10000 rows), total rows: 250000\n",
      "Processed batch 26 (10000 rows), total rows: 260000\n",
      "Processed batch 27 (10000 rows), total rows: 270000\n",
      "Processed batch 28 (10000 rows), total rows: 280000\n",
      "Processed batch 29 (10000 rows), total rows: 290000\n",
      "Processed batch 30 (10000 rows), total rows: 300000\n",
      "Processed batch 31 (10000 rows), total rows: 310000\n",
      "Processed batch 32 (10000 rows), total rows: 320000\n",
      "Processed batch 33 (10000 rows), total rows: 330000\n",
      "Processed batch 34 (10000 rows), total rows: 340000\n",
      "Processed batch 35 (10000 rows), total rows: 350000\n",
      "Processed batch 36 (10000 rows), total rows: 360000\n",
      "Processed batch 37 (10000 rows), total rows: 370000\n",
      "Processed batch 38 (10000 rows), total rows: 380000\n",
      "Processed batch 39 (10000 rows), total rows: 390000\n",
      "Processed batch 40 (10000 rows), total rows: 400000\n",
      "Processed batch 41 (10000 rows), total rows: 410000\n",
      "Processed batch 42 (10000 rows), total rows: 420000\n",
      "Processed batch 43 (10000 rows), total rows: 430000\n",
      "Processed batch 44 (10000 rows), total rows: 440000\n",
      "Processed batch 45 (10000 rows), total rows: 450000\n",
      "Processed batch 46 (10000 rows), total rows: 460000\n",
      "Processed batch 47 (10000 rows), total rows: 470000\n",
      "Processed batch 48 (10000 rows), total rows: 480000\n",
      "Processed batch 49 (10000 rows), total rows: 490000\n",
      "Processed batch 50 (10000 rows), total rows: 500000\n",
      "Processed batch 51 (10000 rows), total rows: 510000\n",
      "Processed batch 52 (10000 rows), total rows: 520000\n",
      "Processed batch 53 (10000 rows), total rows: 530000\n",
      "Processed batch 54 (10000 rows), total rows: 540000\n",
      "Processed batch 55 (10000 rows), total rows: 550000\n",
      "Processed batch 56 (10000 rows), total rows: 560000\n",
      "Processed batch 57 (10000 rows), total rows: 570000\n",
      "Processed batch 58 (10000 rows), total rows: 580000\n",
      "Processed batch 59 (10000 rows), total rows: 590000\n",
      "Processed batch 60 (10000 rows), total rows: 600000\n",
      "Processed batch 61 (10000 rows), total rows: 610000\n",
      "Processed batch 62 (10000 rows), total rows: 620000\n",
      "Processed batch 63 (10000 rows), total rows: 630000\n",
      "Processed batch 64 (10000 rows), total rows: 640000\n",
      "Processed batch 65 (10000 rows), total rows: 650000\n",
      "Processed batch 66 (10000 rows), total rows: 660000\n",
      "Processed batch 67 (10000 rows), total rows: 670000\n",
      "Processed batch 68 (10000 rows), total rows: 680000\n",
      "Processed batch 69 (10000 rows), total rows: 690000\n",
      "Processed batch 70 (10000 rows), total rows: 700000\n",
      "Processed batch 71 (10000 rows), total rows: 710000\n",
      "Processed batch 72 (10000 rows), total rows: 720000\n",
      "Processed batch 73 (10000 rows), total rows: 730000\n",
      "Processed batch 74 (10000 rows), total rows: 740000\n",
      "Processed batch 75 (10000 rows), total rows: 750000\n",
      "Processed batch 76 (10000 rows), total rows: 760000\n",
      "Processed batch 77 (10000 rows), total rows: 770000\n",
      "Processed batch 78 (10000 rows), total rows: 780000\n",
      "Processed batch 79 (10000 rows), total rows: 790000\n",
      "Processed batch 80 (10000 rows), total rows: 800000\n",
      "Processed batch 81 (10000 rows), total rows: 810000\n",
      "Processed batch 82 (10000 rows), total rows: 820000\n",
      "Processed batch 83 (10000 rows), total rows: 830000\n",
      "Processed batch 84 (10000 rows), total rows: 840000\n",
      "Processed batch 85 (10000 rows), total rows: 850000\n",
      "Processed batch 86 (10000 rows), total rows: 860000\n",
      "Processed batch 87 (10000 rows), total rows: 870000\n",
      "Processed batch 88 (10000 rows), total rows: 880000\n",
      "Processed batch 89 (10000 rows), total rows: 890000\n",
      "Processed batch 90 (10000 rows), total rows: 900000\n",
      "Processed batch 91 (10000 rows), total rows: 910000\n",
      "Processed batch 92 (10000 rows), total rows: 920000\n",
      "Processed batch 93 (10000 rows), total rows: 930000\n",
      "Processed batch 94 (10000 rows), total rows: 940000\n",
      "Processed batch 95 (10000 rows), total rows: 950000\n",
      "Processed batch 96 (10000 rows), total rows: 960000\n",
      "Processed batch 97 (10000 rows), total rows: 970000\n",
      "Processed batch 98 (10000 rows), total rows: 980000\n",
      "Processed batch 99 (10000 rows), total rows: 990000\n",
      "Processed batch 100 (10000 rows), total rows: 1000000\n",
      "Processed batch 101 (10000 rows), total rows: 1010000\n",
      "Processed batch 102 (10000 rows), total rows: 1020000\n",
      "Processed batch 103 (10000 rows), total rows: 1030000\n",
      "Processed batch 104 (10000 rows), total rows: 1040000\n",
      "Processed batch 105 (10000 rows), total rows: 1050000\n",
      "Processed batch 106 (10000 rows), total rows: 1060000\n",
      "Processed batch 107 (10000 rows), total rows: 1070000\n",
      "Processed batch 108 (10000 rows), total rows: 1080000\n",
      "Processed batch 109 (10000 rows), total rows: 1090000\n",
      "Processed batch 110 (10000 rows), total rows: 1100000\n",
      "Processed batch 111 (10000 rows), total rows: 1110000\n",
      "Processed batch 112 (10000 rows), total rows: 1120000\n",
      "Processed batch 113 (10000 rows), total rows: 1130000\n",
      "Processed batch 114 (10000 rows), total rows: 1140000\n",
      "Processed batch 115 (10000 rows), total rows: 1150000\n",
      "Processed batch 116 (10000 rows), total rows: 1160000\n",
      "Processed batch 117 (10000 rows), total rows: 1170000\n",
      "Processed batch 118 (10000 rows), total rows: 1180000\n",
      "Processed batch 119 (10000 rows), total rows: 1190000\n",
      "Processed batch 120 (10000 rows), total rows: 1200000\n",
      "Processed batch 121 (10000 rows), total rows: 1210000\n",
      "Processed batch 122 (10000 rows), total rows: 1220000\n",
      "Processed batch 123 (10000 rows), total rows: 1230000\n",
      "Processed batch 124 (10000 rows), total rows: 1240000\n",
      "Processed batch 125 (10000 rows), total rows: 1250000\n",
      "Processed batch 126 (10000 rows), total rows: 1260000\n",
      "Processed batch 127 (10000 rows), total rows: 1270000\n",
      "Processed batch 128 (10000 rows), total rows: 1280000\n",
      "Processed batch 129 (10000 rows), total rows: 1290000\n",
      "Processed batch 130 (10000 rows), total rows: 1300000\n",
      "Processed batch 131 (10000 rows), total rows: 1310000\n",
      "Processed batch 132 (10000 rows), total rows: 1320000\n",
      "Processed batch 133 (10000 rows), total rows: 1330000\n",
      "Processed batch 134 (10000 rows), total rows: 1340000\n",
      "Processed batch 135 (10000 rows), total rows: 1350000\n",
      "Processed batch 136 (10000 rows), total rows: 1360000\n",
      "Processed batch 137 (10000 rows), total rows: 1370000\n",
      "Processed batch 138 (10000 rows), total rows: 1380000\n",
      "Processed batch 139 (10000 rows), total rows: 1390000\n",
      "Processed batch 140 (10000 rows), total rows: 1400000\n",
      "Processed batch 141 (10000 rows), total rows: 1410000\n",
      "Processed batch 142 (10000 rows), total rows: 1420000\n",
      "Processed batch 143 (10000 rows), total rows: 1430000\n",
      "Processed batch 144 (10000 rows), total rows: 1440000\n",
      "Processed batch 145 (10000 rows), total rows: 1450000\n",
      "Processed batch 146 (10000 rows), total rows: 1460000\n",
      "Processed batch 147 (10000 rows), total rows: 1470000\n",
      "Processed batch 148 (10000 rows), total rows: 1480000\n",
      "Processed batch 149 (10000 rows), total rows: 1490000\n",
      "Processed batch 150 (10000 rows), total rows: 1500000\n",
      "Processed batch 151 (10000 rows), total rows: 1510000\n",
      "Processed batch 152 (10000 rows), total rows: 1520000\n",
      "Processed batch 153 (10000 rows), total rows: 1530000\n",
      "Processed batch 154 (10000 rows), total rows: 1540000\n",
      "Processed batch 155 (10000 rows), total rows: 1550000\n",
      "Processed batch 156 (10000 rows), total rows: 1560000\n",
      "Processed batch 157 (10000 rows), total rows: 1570000\n",
      "Processed batch 158 (10000 rows), total rows: 1580000\n",
      "Processed batch 159 (10000 rows), total rows: 1590000\n",
      "Processed batch 160 (10000 rows), total rows: 1600000\n",
      "Processed batch 161 (10000 rows), total rows: 1610000\n",
      "Processed batch 162 (10000 rows), total rows: 1620000\n",
      "Processed batch 163 (10000 rows), total rows: 1630000\n",
      "Processed batch 164 (10000 rows), total rows: 1640000\n",
      "Processed batch 165 (10000 rows), total rows: 1650000\n",
      "Processed batch 166 (10000 rows), total rows: 1660000\n",
      "Processed batch 167 (10000 rows), total rows: 1670000\n",
      "Processed batch 168 (10000 rows), total rows: 1680000\n",
      "Processed batch 169 (10000 rows), total rows: 1690000\n",
      "Processed batch 170 (10000 rows), total rows: 1700000\n",
      "Processed batch 171 (10000 rows), total rows: 1710000\n",
      "Processed batch 172 (10000 rows), total rows: 1720000\n",
      "Processed batch 173 (10000 rows), total rows: 1730000\n",
      "Processed batch 174 (10000 rows), total rows: 1740000\n",
      "Processed batch 175 (10000 rows), total rows: 1750000\n",
      "Processed batch 176 (10000 rows), total rows: 1760000\n",
      "Processed batch 177 (10000 rows), total rows: 1770000\n",
      "Processed batch 178 (10000 rows), total rows: 1780000\n",
      "Processed batch 179 (10000 rows), total rows: 1790000\n",
      "Processed batch 180 (10000 rows), total rows: 1800000\n",
      "Processed batch 181 (10000 rows), total rows: 1810000\n",
      "Processed batch 182 (10000 rows), total rows: 1820000\n",
      "Processed batch 183 (10000 rows), total rows: 1830000\n",
      "Processed batch 184 (10000 rows), total rows: 1840000\n",
      "Processed batch 185 (10000 rows), total rows: 1850000\n",
      "Processed batch 186 (10000 rows), total rows: 1860000\n",
      "Processed batch 187 (10000 rows), total rows: 1870000\n",
      "Processed batch 188 (10000 rows), total rows: 1880000\n",
      "Processed batch 189 (10000 rows), total rows: 1890000\n",
      "Processed batch 190 (10000 rows), total rows: 1900000\n",
      "Processed batch 191 (10000 rows), total rows: 1910000\n",
      "Processed batch 192 (10000 rows), total rows: 1920000\n",
      "Processed batch 193 (10000 rows), total rows: 1930000\n",
      "Processed batch 194 (10000 rows), total rows: 1940000\n",
      "Processed batch 195 (10000 rows), total rows: 1950000\n",
      "Processed batch 196 (10000 rows), total rows: 1960000\n",
      "Processed batch 197 (10000 rows), total rows: 1970000\n",
      "Processed batch 198 (10000 rows), total rows: 1980000\n",
      "Processed batch 199 (10000 rows), total rows: 1990000\n",
      "Processed batch 200 (10000 rows), total rows: 2000000\n",
      "Processed batch 201 (10000 rows), total rows: 2010000\n",
      "Processed batch 202 (10000 rows), total rows: 2020000\n",
      "Processed batch 203 (10000 rows), total rows: 2030000\n",
      "Processed batch 204 (10000 rows), total rows: 2040000\n",
      "Processed batch 205 (10000 rows), total rows: 2050000\n",
      "Processed batch 206 (10000 rows), total rows: 2060000\n",
      "Processed batch 207 (10000 rows), total rows: 2070000\n",
      "Processed batch 208 (10000 rows), total rows: 2080000\n",
      "Processed batch 209 (10000 rows), total rows: 2090000\n",
      "Processed batch 210 (10000 rows), total rows: 2100000\n",
      "Processed batch 211 (10000 rows), total rows: 2110000\n",
      "Processed batch 212 (10000 rows), total rows: 2120000\n",
      "Processed batch 213 (10000 rows), total rows: 2130000\n",
      "Processed batch 214 (10000 rows), total rows: 2140000\n",
      "Processed batch 215 (10000 rows), total rows: 2150000\n",
      "Processed batch 216 (10000 rows), total rows: 2160000\n",
      "Processed batch 217 (10000 rows), total rows: 2170000\n",
      "Processed batch 218 (10000 rows), total rows: 2180000\n",
      "Processed batch 219 (10000 rows), total rows: 2190000\n",
      "Processed batch 220 (10000 rows), total rows: 2200000\n",
      "Processed batch 221 (10000 rows), total rows: 2210000\n",
      "Processed batch 222 (10000 rows), total rows: 2220000\n",
      "Processed batch 223 (10000 rows), total rows: 2230000\n",
      "Processed batch 224 (10000 rows), total rows: 2240000\n",
      "Processed batch 225 (10000 rows), total rows: 2250000\n",
      "Processed batch 226 (10000 rows), total rows: 2260000\n",
      "Processed batch 227 (10000 rows), total rows: 2270000\n",
      "Processed batch 228 (10000 rows), total rows: 2280000\n",
      "Processed batch 229 (10000 rows), total rows: 2290000\n",
      "Processed batch 230 (10000 rows), total rows: 2300000\n",
      "Processed batch 231 (10000 rows), total rows: 2310000\n",
      "Processed batch 232 (10000 rows), total rows: 2320000\n",
      "Processed batch 233 (10000 rows), total rows: 2330000\n",
      "Processed batch 234 (10000 rows), total rows: 2340000\n",
      "Processed batch 235 (10000 rows), total rows: 2350000\n",
      "Processed batch 236 (10000 rows), total rows: 2360000\n",
      "Processed batch 237 (10000 rows), total rows: 2370000\n",
      "Processed batch 238 (10000 rows), total rows: 2380000\n",
      "Processed batch 239 (10000 rows), total rows: 2390000\n",
      "Processed batch 240 (10000 rows), total rows: 2400000\n",
      "Processed batch 241 (10000 rows), total rows: 2410000\n",
      "Processed batch 242 (10000 rows), total rows: 2420000\n",
      "Processed batch 243 (10000 rows), total rows: 2430000\n",
      "Processed batch 244 (10000 rows), total rows: 2440000\n",
      "Processed batch 245 (10000 rows), total rows: 2450000\n",
      "Processed batch 246 (10000 rows), total rows: 2460000\n",
      "Processed batch 247 (10000 rows), total rows: 2470000\n",
      "Processed batch 248 (10000 rows), total rows: 2480000\n",
      "Processed batch 249 (10000 rows), total rows: 2490000\n",
      "Processed batch 250 (10000 rows), total rows: 2500000\n",
      "Processed batch 251 (10000 rows), total rows: 2510000\n",
      "Processed batch 252 (10000 rows), total rows: 2520000\n",
      "Processed batch 253 (10000 rows), total rows: 2530000\n",
      "Processed batch 254 (10000 rows), total rows: 2540000\n",
      "Processed batch 255 (10000 rows), total rows: 2550000\n",
      "Processed batch 256 (10000 rows), total rows: 2560000\n",
      "Processed batch 257 (10000 rows), total rows: 2570000\n",
      "Processed batch 258 (10000 rows), total rows: 2580000\n",
      "Processed batch 259 (10000 rows), total rows: 2590000\n",
      "Processed batch 260 (10000 rows), total rows: 2600000\n",
      "Processed batch 261 (10000 rows), total rows: 2610000\n",
      "Processed batch 262 (10000 rows), total rows: 2620000\n",
      "Processed batch 263 (10000 rows), total rows: 2630000\n",
      "Processed batch 264 (10000 rows), total rows: 2640000\n",
      "Processed batch 265 (10000 rows), total rows: 2650000\n",
      "Processed batch 266 (10000 rows), total rows: 2660000\n",
      "Processed batch 267 (10000 rows), total rows: 2670000\n",
      "Processed batch 268 (10000 rows), total rows: 2680000\n",
      "Processed batch 269 (10000 rows), total rows: 2690000\n",
      "Processed batch 270 (10000 rows), total rows: 2700000\n",
      "Processed batch 271 (10000 rows), total rows: 2710000\n",
      "Processed batch 272 (10000 rows), total rows: 2720000\n",
      "Processed batch 273 (10000 rows), total rows: 2730000\n",
      "Processed batch 274 (8586 rows), total rows: 2738586\n",
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------- GADM 2 BATCH -----------------------------------------------------------------\n",
    "\n",
    "import psycopg2\n",
    "from urllib.parse import quote_plus\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import csv\n",
    "import os\n",
    "\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=database,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    host=host,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "cur = conn.cursor(name='parquet_cursor')  # server-side cursor\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "     select extract(year from ge.start_time) as year \n",
    "        , gb1.gadm_id gadm_1_id\n",
    "        , gb1.name gadm_1_name\n",
    "        , gb1.corrected_name gadm_1_corrected_name\n",
    "        , ge.gadm_id gadm_2_id\n",
    "        , gb2.name gadm_2_name\n",
    "        , gb2.corrected_name gadm_2_corrected_name\n",
    "        , gb2.gid\n",
    "        , gb2.admin_level\n",
    "        , ge.iso3_country\n",
    "        , ca.name as country_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , ge.original_inventory_sector subsector\n",
    "        , sum(asset_activity) asset_activity\n",
    "        , sum(asset_emissions) asset_emissions\n",
    "        , sum(remainder_activity) remainder_activity\n",
    "        , sum(remainder_emissions) remainder_emissions\n",
    "        , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "    from gadm_emissions ge\n",
    "    inner join (\n",
    "        select distinct gadm_id\n",
    "            , gid\n",
    "            , immediate_parent\n",
    "            , name\n",
    "            , corrected_name\n",
    "            , admin_level\n",
    "        from gadm_boundaries\n",
    "        where admin_level = 2\n",
    "    ) as gb2\n",
    "        on ge.gadm_id = gb2.gadm_id\n",
    "    left join (\n",
    "        select distinct sector\n",
    "            , subsector\n",
    "        from asset_schema\n",
    "    ) asch\n",
    "        on cast(asch.subsector as varchar) = cast(ge.original_inventory_sector as varchar)\n",
    "    left join (\n",
    "        select gadm_id\n",
    "            , name\n",
    "            , corrected_name\n",
    "        from gadm_boundaries\n",
    "        where admin_level = 1\n",
    "    ) gb1\n",
    "        on gb1.gadm_id = gb2.immediate_parent\n",
    "    left join country_analysis ca\n",
    "        on cast(ca.iso3_country as varchar) = cast(ge.iso3_country as varchar)\n",
    "\n",
    "    where ge.gas = 'co2e_100yr'\n",
    "        and extract(year from start_time) = 2024\n",
    "        and ge.original_inventory_sector not in ('forest-land-clearing',\n",
    "                                                'forest-land-degradation',\n",
    "                                                'forest-land-fires',\n",
    "                                                'net-forest-land',\n",
    "                                                'net-shrubgrass',\n",
    "                                                'net-wetland',\n",
    "                                                'removals',\n",
    "                                                'shrubgrass-fires',\n",
    "                                                'water-reservoirs',\n",
    "                                                'wetland-fires')\n",
    "\n",
    "    group by extract(year from ge.start_time)\n",
    "        , gb1.gadm_id \n",
    "        , gb1.name\n",
    "        , gb1.corrected_name\n",
    "        , ge.gadm_id \n",
    "        , gb2.name\n",
    "        , gb2.corrected_name\n",
    "        , gb2.gid\n",
    "        , gb2.admin_level\n",
    "        , ge.iso3_country\n",
    "        , ca.name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , ge.original_inventory_sector\n",
    "    \"\"\")\n",
    "\n",
    "# Set up Parquet writer\n",
    "batch_size = 10000\n",
    "output_file = \"data/emissions_reduction/gadm_2_emissions.parquet\"\n",
    "batch_count = 0\n",
    "total_rows = 0\n",
    "\n",
    "print(\"executing gadm_2 query...\")\n",
    "\n",
    "# Fetch first batch\n",
    "rows = cur.fetchmany(batch_size)\n",
    "if not rows:\n",
    "    raise Exception(\"No data returned from query.\")\n",
    "\n",
    "field_names = [desc[0] for desc in cur.description]\n",
    "first_table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "writer = pq.ParquetWriter(output_file, first_table.schema)\n",
    "writer.write_table(first_table)\n",
    "batch_count += 1\n",
    "total_rows += len(rows)\n",
    "print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "# Process remaining batches\n",
    "while True:\n",
    "    rows = cur.fetchmany(batch_size)\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "    table = table.cast(writer.schema)  # ensure schema matches first batch\n",
    "    writer.write_table(table)\n",
    "\n",
    "    batch_count += 1\n",
    "    total_rows += len(rows)\n",
    "    print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "writer.close()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c5914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ GADM_0 Emissions ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/emissions_reduction/gadm_0_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "print('Running query')\n",
    "con.execute(f'''\n",
    "    INSTALL postgres;\n",
    "    LOAD postgres;\n",
    "\n",
    "    CREATE TABLE gadm_0_emissions_parquet AS\n",
    "    select extract(year from g0e.start_time) as year \n",
    "        , g0e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g0e.iso3_country\n",
    "        , ca.name as country_name\n",
    "        , gb.name gadm_0_name\n",
    "        , gb.corrected_name gadm_0_corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g0e.original_inventory_sector subsector\n",
    "        , g0e.gas\n",
    "        , sum(asset_activity) asset_activity\n",
    "        , sum(asset_emissions) asset_emissions\n",
    "        , sum(remainder_activity) remainder_activity\n",
    "        , sum(remainder_emissions) remainder_emissions\n",
    "        , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "    from postgres_scan('{postgres_url}', 'public', 'gadm_0_emissions') g0e\n",
    "    inner join (\n",
    "        select distinct gadm_id\n",
    "            , gid\n",
    "            , name\n",
    "            , corrected_name\n",
    "            , admin_level\n",
    "        from postgres_scan('{postgres_url}','public', 'gadm_boundaries') \n",
    "        where admin_level = 0\n",
    "    ) as gb\n",
    "        on g0e.gadm_id = gb.gadm_id\n",
    "    left join (\n",
    "        select distinct sector\n",
    "            , subsector\n",
    "        from postgres_scan('{postgres_url}','public', 'asset_schema') \n",
    "    ) asch\n",
    "        on cast(asch.subsector as varchar) = cast(g0e.original_inventory_sector as varchar)\n",
    "    left join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(g0e.iso3_country as varchar)\n",
    "\n",
    "    where g0e.gas = 'co2e_100yr'\n",
    "        and extract(year from start_time) = 2024\n",
    "        and g0e.original_inventory_sector not in ('forest-land-clearing',\n",
    "                                                'forest-land-degradation',\n",
    "                                                'forest-land-fires',\n",
    "                                                'net-forest-land',\n",
    "                                                'net-shrubgrass',\n",
    "                                                'net-wetland',\n",
    "                                                'removals',\n",
    "                                                'shrubgrass-fires',\n",
    "                                                'water-reservoirs',\n",
    "                                                'wetland-fires')\n",
    "\n",
    "    group by extract(year from g0e.start_time) \n",
    "        , g0e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g0e.iso3_country\n",
    "        , ca.name\n",
    "        , gb.name \n",
    "        , gb.corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g0e.original_inventory_sector\n",
    "        , g0e.gas;\n",
    "\n",
    "    COPY gadm_0_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "''')\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c14e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ City Emissions ------------------------------------\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/emissions_reduction/city_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "print('Running query...')\n",
    "con.execute( f'''\n",
    "\tINSTALL postgres;\n",
    "\tLOAD postgres;\n",
    "\n",
    "\tCREATE TABLE city_emissions_parquet AS\n",
    "    \n",
    "\tselect extract(year from start_time) as year\n",
    "\t\t, ce.city_id\n",
    "\t\t, cb.name as city_name\n",
    "\t\t, cb.corrected_name as corrected_name\n",
    "\t\t, ce.iso3_country\n",
    "\t\t, ca.name as country_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ce.original_inventory_sector as subsector\n",
    "\t\t, sum(asset_activity) asset_activity\n",
    "\t\t, sum(asset_emissions) asset_emissions\n",
    "\t\t, sum(remainder_activity) remainder_activity\n",
    "\t\t, sum(remainder_emissions) remainder_emissions\n",
    "\t\t, sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "\tfrom postgres_scan('{postgres_url}','public', 'city_emissions') ce\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'city_boundaries') cb\n",
    "\t\ton cb.city_id = ce.city_id\n",
    "        and cb.reporting_entity = 'ghs-fua'\n",
    "\tleft join (\n",
    "\t\tselect distinct sector, subsector\n",
    "\t\tfrom postgres_scan('{postgres_url}','public', 'asset_schema')\n",
    "\t) asch\n",
    "\t\ton cast(asch.subsector as varchar) = cast(ce.original_inventory_sector as varchar)\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(ce.iso3_country as varchar)\n",
    "\n",
    "\twhere extract(year from ce.start_time) = 2024\n",
    "\t\tand ce.gas = 'co2e_100yr'\n",
    "\t\tand ce.original_inventory_sector not in ('forest-land-clearing',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-degradation',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'net-forest-land',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'net-shrubgrass',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'net-wetland',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'removals',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'shrubgrass-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'water-reservoirs',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'wetland-fires')\n",
    "\n",
    "\tgroup by extract(year from start_time) \n",
    "\t\t, ce.city_id\n",
    "\t\t, cb.name \n",
    "\t\t, cb.corrected_name \n",
    "\t\t, ce.iso3_country\n",
    "\t\t, ca.name \n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ce.original_inventory_sector;\n",
    "            \n",
    "    COPY city_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "            \n",
    "    ''')\n",
    "\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d0808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------ GADM 2 Emissions ------------------------------------\n",
    "\n",
    "# import duckdb\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from urllib.parse import quote_plus\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# # Build SQLAlchemy engine for PostgreSQL\n",
    "# user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "# password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "# host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "# port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "# database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "# postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "# parquet_path = \"data/emissions_reduction/gadm_2_emissions.parquet\"\n",
    "\n",
    "# # Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "# con = duckdb.connect()\n",
    "\n",
    "\n",
    "# print('Running query')\n",
    "# con.execute(f'''\n",
    "#     INSTALL postgres;\n",
    "#     LOAD postgres;\n",
    "\n",
    "#     CREATE TABLE gadm_2_emissions_parquet AS\n",
    "#     select extract(year from ge.start_time) as year \n",
    "#         , gb1.gadm_id gadm_1_id\n",
    "#         , gb1.name gadm_1_name\n",
    "#         , gb1.corrected_name gadm_1_corrected_name\n",
    "#         , ge.gadm_id gadm_2_id\n",
    "#         , gb2.name gadm_2_name\n",
    "#         , gb2.corrected_name gadm_2_corrected_name\n",
    "#         , gb2.admin_level\n",
    "#         , ge.iso3_country\n",
    "#         , ca.name as country_name\n",
    "#         , ca.continent\n",
    "#         , ca.eu\n",
    "#         , ca.oecd\n",
    "#         , ca.unfccc_annex\n",
    "#         , ca.developed_un\n",
    "#         , ca.em_finance\n",
    "#         , asch.sector\n",
    "#         , ge.original_inventory_sector subsector\n",
    "#         , ge.gas\n",
    "#         , sum(asset_activity) asset_activity\n",
    "#         , sum(asset_emissions) asset_emissions\n",
    "#         , sum(remainder_activity) remainder_activity\n",
    "#         , sum(remainder_emissions) remainder_emissions\n",
    "#         , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "#     from postgres_scan('{postgres_url}','public', 'gadm_emissions') ge\n",
    "#     inner join (\n",
    "#         select distinct gadm_id\n",
    "#             , immediate_parent\n",
    "#             , name\n",
    "#             , corrected_name\n",
    "#             , admin_level\n",
    "#         from postgres_scan('{postgres_url}','public', 'gadm_boundaries')\n",
    "#         where admin_level = 2\n",
    "#     ) as gb2\n",
    "#         on ge.gadm_id = gb2.gadm_id\n",
    "#     left join (\n",
    "#         select distinct sector\n",
    "#             , subsector\n",
    "#         from postgres_scan('{postgres_url}','public', 'asset_schema')\n",
    "#     ) asch\n",
    "#         on cast(asch.subsector as varchar) = cast(ge.original_inventory_sector as varchar)\n",
    "#     left join (\n",
    "#         select gadm_id\n",
    "#             , name\n",
    "#             , corrected_name\n",
    "#         from postgres_scan('{postgres_url}','public', 'gadm_boundaries')\n",
    "#         where admin_level = 1\n",
    "#     ) gb1\n",
    "#         on gb1.gadm_id = gb2.immediate_parent\n",
    "#     left join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "#         on cast(ca.iso3_country as varchar) = cast(ge.iso3_country as varchar)\n",
    "\n",
    "#     where ge.gas = 'co2e_100yr'\n",
    "#         and extract(year from start_time) = 2024\n",
    "#         and ge.original_inventory_sector not in ('forest-land-clearing',\n",
    "#                                                 'forest-land-degradation',\n",
    "#                                                 'forest-land-fires',\n",
    "#                                                 'net-forest-land',\n",
    "#                                                 'net-shrubgrass',\n",
    "#                                                 'net-wetland',\n",
    "#                                                 'removals',\n",
    "#                                                 'shrubgrass-fires',\n",
    "#                                                 'water-reservoirs',\n",
    "#                                                 'wetland-fires')\n",
    "\n",
    "#     group by extract(year from ge.start_time)\n",
    "#         , gb1.gadm_id \n",
    "#         , gb1.name\n",
    "#         , gb1.corrected_name\n",
    "#         , ge.gadm_id \n",
    "#         , gb2.name\n",
    "#         , gb2.corrected_name\n",
    "#         , gb2.admin_level\n",
    "#         , ge.iso3_country\n",
    "#         , ca.name\n",
    "#         , ca.continent\n",
    "#         , ca.eu\n",
    "#         , ca.oecd\n",
    "#         , ca.unfccc_annex\n",
    "#         , ca.developed_un\n",
    "#         , ca.em_finance\n",
    "#         , asch.sector\n",
    "#         , ge.original_inventory_sector\n",
    "#         , ge.gas;\n",
    "\n",
    "#     COPY gadm_2_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "# ''')\n",
    "# con.close()\n",
    "\n",
    "# print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4462d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------------- CITY BATCH -----------------------------------------------------------------\n",
    "\n",
    "# import psycopg2\n",
    "# from urllib.parse import quote_plus\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import os\n",
    "\n",
    "# user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "# password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "# host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "# port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "# database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "# conn = psycopg2.connect(\n",
    "#     dbname=database,\n",
    "#     user=user,\n",
    "#     password=password,\n",
    "#     host=host,\n",
    "#     port=port\n",
    "# )\n",
    "\n",
    "# cur = conn.cursor(name='parquet_cursor')  # server-side cursor\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "#     SELECT extract(year from start_time) AS year,\n",
    "#            ce.city_id,\n",
    "#            cb.name AS city_name,\n",
    "#            cb.corrected_name AS corrected_name,\n",
    "#            ce.iso3_country,\n",
    "#            ca.name AS country_name,\n",
    "#            ca.continent,\n",
    "#            ca.eu,\n",
    "#            ca.oecd,\n",
    "#            ca.unfccc_annex,\n",
    "#            ca.developed_un,\n",
    "#            ca.em_finance,\n",
    "#            asch.sector,\n",
    "#            ce.original_inventory_sector AS subsector,\n",
    "#            SUM(asset_activity) AS asset_activity,\n",
    "#            SUM(asset_emissions) AS asset_emissions,\n",
    "#            SUM(remainder_activity) AS remainder_activity,\n",
    "#            SUM(remainder_emissions) AS remainder_emissions,\n",
    "#            SUM(asset_emissions) + SUM(remainder_emissions) AS emissions_quantity\n",
    "#     FROM city_emissions ce\n",
    "#     LEFT JOIN city_boundaries cb ON cb.city_id = ce.city_id\n",
    "#     LEFT JOIN (\n",
    "#         SELECT DISTINCT sector, subsector FROM asset_schema\n",
    "#     ) asch ON CAST(asch.subsector AS varchar) = CAST(ce.original_inventory_sector AS varchar)\n",
    "#     LEFT JOIN country_analysis ca ON CAST(ca.iso3_country AS varchar) = CAST(ce.iso3_country AS varchar)\n",
    "#     WHERE extract(year FROM ce.start_time) = 2024\n",
    "#       AND ce.gas = 'co2e_100yr'\n",
    "#       AND ce.original_inventory_sector NOT IN (\n",
    "#           'forest-land-clearing', 'forest-land-degradation', 'forest-land-fires',\n",
    "#           'net-forest-land', 'net-shrubgrass', 'net-wetland', 'removals',\n",
    "#           'shrubgrass-fires', 'water-reservoirs', 'wetland-fires'\n",
    "#       )\n",
    "#     GROUP BY extract(year FROM start_time),\n",
    "#              ce.city_id, cb.name, cb.corrected_name,\n",
    "#              ce.iso3_country, ca.name, ca.continent, ca.eu, ca.oecd,\n",
    "#              ca.unfccc_annex, ca.developed_un, ca.em_finance,\n",
    "#              asch.sector, ce.original_inventory_sector\n",
    "# \"\"\")\n",
    "\n",
    "# # Set up Parquet writer\n",
    "# batch_size = 10000\n",
    "# output_file = \"data/emissions_reduction/city_emissions.parquet\"\n",
    "# batch_count = 0\n",
    "# total_rows = 0\n",
    "\n",
    "# print(\"executing city query...\")\n",
    "\n",
    "# # Fetch first batch\n",
    "# rows = cur.fetchmany(batch_size)\n",
    "# if not rows:\n",
    "#     raise Exception(\"No data returned from query.\")\n",
    "\n",
    "# field_names = [desc[0] for desc in cur.description]\n",
    "# first_table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "# writer = pq.ParquetWriter(output_file, first_table.schema)\n",
    "# writer.write_table(first_table)\n",
    "# batch_count += 1\n",
    "# total_rows += len(rows)\n",
    "# print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "# # Process remaining batches\n",
    "# while True:\n",
    "#     rows = cur.fetchmany(batch_size)\n",
    "#     if not rows:\n",
    "#         break\n",
    "\n",
    "#     table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "#     table = table.cast(writer.schema)  # ensure schema matches first batch\n",
    "#     writer.write_table(table)\n",
    "\n",
    "#     batch_count += 1\n",
    "#     total_rows += len(rows)\n",
    "#     print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "# writer.close()\n",
    "# cur.close()\n",
    "# conn.close()\n",
    "# print(\"Export complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01348880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import duckdb\n",
    "\n",
    "# con = duckdb.connect()\n",
    "# asset_annual_path = 'data/asset_annual_emissions/chunk_*.parquet'\n",
    "# output_path = 'data/fixed_aluminum_moer.parquet'\n",
    "\n",
    "# con.execute(f\"\"\"\n",
    "#                     copy(\n",
    "#                         select year\n",
    "#                             , asset_id\n",
    "#                             , asset_type\n",
    "#                             , asset_name\n",
    "#                             , iso3_country\n",
    "#                             , country_name\n",
    "#                             , continent\n",
    "#                             , eu\n",
    "#                             , oecd\n",
    "#                             , unfccc_annex\n",
    "#                             , developed_un\n",
    "#                             , em_finance\n",
    "#                             , sector\n",
    "#                             , subsector\n",
    "#                             , gadm_1\n",
    "#                             , gadm_2\n",
    "#                             , ghs_fua\n",
    "#                             , city_id\n",
    "#                             , other1\n",
    "#                             , other2\n",
    "#                             , other3\n",
    "#                             , other4\n",
    "#                             , other5\n",
    "#                             , other6\n",
    "#                             , other7\n",
    "#                             , other8\n",
    "#                             , other9\n",
    "#                             , other10\n",
    "#                             , activity_units\n",
    "#                             , capacity\n",
    "#                             , activity\n",
    "#                             , average_emissions_factor\n",
    "#                             , emissions_quantity\n",
    "#                             , case when subsector = 'aluminum' then null else ae.ef_moer end as ef_moer\n",
    "#                             , case when subsector = 'aluminum' then null else ae.eq_12 end as eq_12\n",
    "#                             , case when subsector = 'aluminum' then null else ae.ef_12 end as ef_12\n",
    "#                             , case when subsector = 'aluminum' then null else ae.eq_12_moer end as eq_12_moer\n",
    "#                             , case when ae.subsector = 'aluminum' then null else ef_12_moer end as ef_12_moer\n",
    "#                             , asset_type_2\n",
    "                        \n",
    "#                         from '{asset_annual_path}' ae\n",
    "#                     ) to '{output_path}' (format 'parquet');\n",
    "#                 \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0707b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/static/ct_percentile_40sectors_moer_stat_industrial_20250824.csv to /Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/static/ct_percentile_40sectors_moer_stat_industrial_20250824.parquet\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Replace with your CSV file path\n",
    "# csv_file = \"/Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/static/ct_percentile_40sectors_moer_stat_industrial_20250824.csv\"\n",
    "# parquet_file = \"/Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/static/ct_percentile_40sectors_moer_stat_industrial_20250824.parquet\"\n",
    "\n",
    "# # Read CSV\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Save as Parquet\n",
    "# df.to_parquet(parquet_file, index=False)\n",
    "\n",
    "# print(f\"Converted {csv_file} to {parquet_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c135a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to /Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/sector_emissions_reduction_summary_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from config import CONFIG\n",
    "import os\n",
    "\n",
    "# Get the path to the annual asset parquet files\n",
    "annual_asset_path = CONFIG['annual_asset_path']\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Define your SQL query\n",
    "query = f'''\n",
    "                WITH sector_mapping as (\n",
    "                    SELECT distinct sector\n",
    "                        , subsector\n",
    "\n",
    "                    FROM '{annual_asset_path}'\n",
    "                ),\n",
    "                \n",
    "                induced as (\n",
    "                    SELECT sector\n",
    "                        , sum(induced_emissions) as induced_emissions\n",
    "\n",
    "                    FROM (\n",
    "                            SELECT\n",
    "                                sector,\n",
    "                                sum(induced_emissions) AS induced_emissions\n",
    "                            FROM (\n",
    "                                SELECT distinct asset_id\n",
    "                                    , sector_mapping.sector\n",
    "                                    , induced_sector_1 AS induced_subsector\n",
    "                                    , induced_sector_1_induced_emissions AS induced_emissions\n",
    "                                FROM '{annual_asset_path}' aap\n",
    "                                LEFT JOIN sector_mapping\n",
    "                                    on sector_mapping.subsector = aap.induced_sector_1\n",
    "                                WHERE induced_sector_1 IS NOT NULL\n",
    "                            )  \n",
    "\n",
    "                            group by sector\n",
    "                            \n",
    "                            UNION ALL\n",
    "                                \n",
    "                            SELECT\n",
    "                                sector,\n",
    "                                sum(induced_emissions) AS induced_emissions\n",
    "                            FROM (\n",
    "                                SELECT distinct asset_id\n",
    "                                    , sector_mapping.sector\n",
    "                                    , induced_sector_2 AS induced_subsector\n",
    "                                    , induced_sector_2_induced_emissions AS induced_emissions\n",
    "                                FROM '{annual_asset_path}' aap\n",
    "                                LEFT JOIN sector_mapping\n",
    "                                    on sector_mapping.subsector = aap.induced_sector_2\n",
    "                                WHERE induced_sector_2 IS NOT NULL\n",
    "                            )  \n",
    "\n",
    "                            group by sector\n",
    "                                \n",
    "                            UNION ALL\n",
    "                                \n",
    "                            SELECT\n",
    "                                sector,\n",
    "                                sum(induced_emissions) AS induced_emissions\n",
    "                            FROM (\n",
    "                                SELECT distinct asset_id\n",
    "                                    , sector_mapping.sector\n",
    "                                    , induced_sector_3 AS induced_subsector\n",
    "                                    , induced_sector_3_induced_emissions AS induced_emissions\n",
    "                                FROM '{annual_asset_path}' aap\n",
    "                                LEFT JOIN sector_mapping\n",
    "                                    on sector_mapping.subsector = aap.induced_sector_3\n",
    "                                WHERE induced_sector_3 IS NOT NULL\n",
    "                            )  \n",
    "\n",
    "                            group by sector\n",
    "                        )\n",
    "\n",
    "                    GROUP BY sector\n",
    "                ),\n",
    "\n",
    "                asset_reductions as (\n",
    "                    SELECT sector\n",
    "                        , sum(emissions_quantity) emissions_quantity\n",
    "                        , sum(emissions_reduced_at_asset) emissions_reduced_at_asset\n",
    "                \n",
    "                    FROM (\n",
    "                        SELECT asset_id\n",
    "                            , sector\n",
    "                            , subsector\n",
    "                            , sum(emissions_quantity) emissions_quantity\n",
    "                            , emissions_reduced_at_asset\n",
    "\n",
    "                        FROM '{annual_asset_path}'\n",
    "\n",
    "                        GROUP BY asset_id\n",
    "                            , sector\n",
    "                            , subsector\n",
    "                            , emissions_reduced_at_asset\n",
    "                    ) asset\n",
    "\n",
    "                    GROUP BY sector\n",
    "                )\n",
    "\n",
    "                SELECT \n",
    "                    COALESCE(ar.sector, induced.sector) AS sector,\n",
    "                    ar.emissions_quantity,\n",
    "                    induced.induced_emissions,\n",
    "                    ar.emissions_reduced_at_asset,\n",
    "                    \n",
    "                    CASE \n",
    "                        WHEN COALESCE(induced.induced_emissions, 0) > COALESCE(ar.emissions_reduced_at_asset, 0)\n",
    "                        THEN COALESCE(induced.induced_emissions, 0) - COALESCE(ar.emissions_reduced_at_asset, 0)\n",
    "                        ELSE 0 \n",
    "                    END AS induced_emissions,\n",
    "                    \n",
    "                    CASE \n",
    "                        WHEN COALESCE(induced.induced_emissions, 0) < COALESCE(ar.emissions_reduced_at_asset, 0)\n",
    "                        THEN COALESCE(ar.emissions_reduced_at_asset, 0) - COALESCE(induced.induced_emissions, 0)\n",
    "                        ELSE 0 \n",
    "                    END AS emissions_reduction_potential\n",
    "\n",
    "                FROM asset_reductions ar\n",
    "                FULL OUTER JOIN induced\n",
    "                    on induced.sector = ar.sector\n",
    "            '''\n",
    "\n",
    "# Execute and fetch as DataFrame\n",
    "df = con.execute(query).df()\n",
    "\n",
    "# Define output path\n",
    "output_path = os.path.join(\"/Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data\", \"sector_emissions_reduction_summary_2024.csv\")\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved CSV to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from config import CONFIG\n",
    "import os\n",
    "\n",
    "# Get the path to the annual asset parquet files\n",
    "annual_asset_path = CONFIG['annual_asset_path']\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Define your SQL query\n",
    "query = f'''\n",
    "        \n",
    "        select asset_id\n",
    "                , strategy_id\n",
    "                , sum(emissions_quantity) emissions_quantity\n",
    "                , total_emissions_reduced_per_year\n",
    "                , emissions_reduced_at_asset\n",
    "                , induced_sector_1_induced_emissions\n",
    "                , induced_sector_2_induced_emissions\n",
    "                , induced_sector_3_indued_emissions\n",
    "\n",
    "        from '{annual_asset_path}'\n",
    "\n",
    "        group by asset_id\n",
    "                , strategy_id\n",
    "                , total_emissions_reduced_per_year\n",
    "                , emissions_reduced_at_asset\n",
    "                , induced_sector_1_induced_emissions\n",
    "                , induced_sector_2_induced_emissions\n",
    "                , induced_sector_3_indued_emissions\n",
    "\n",
    "        where induced_sector_1 = 'electricity-generation'\n",
    "                or induced_sector_2 = 'electricity-generation'\n",
    "                or induced_sector_3 = 'electricity-generation'\n",
    "        \n",
    "\n",
    "        '''\n",
    "\n",
    "# Execute and fetch as DataFrame\n",
    "df = con.execute(query).df()\n",
    "\n",
    "# Define output path\n",
    "output_path = os.path.join(\"/Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data\", \"induced_electricity_2024.csv\")\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved CSV to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
