{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c52d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting country_subsector_emissions_totals_202509.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dz/vqfc7snj23qc01ndgmlznjtc0000gn/T/ipykernel_2005/1208930217.py:21: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/country_subsector_emissions_totals_202509.parquet\n",
      "Deleted original CSV: country_subsector_emissions_totals_202509.csv\n",
      "Converting country_subsector_emissions_statistics_202509.csv...\n",
      "Saved to data/country_subsector_emissions_statistics_202509.parquet\n",
      "Deleted original CSV: country_subsector_emissions_statistics_202509.csv\n",
      "✅ CSV to Parquet conversion complete.\n"
     ]
    }
   ],
   "source": [
    "######## needs refactor make it a function\n",
    "\n",
    "'''\n",
    "This is a script to take raw csvs in data/raw_csvs folder and covert them to \n",
    "parquets for manageable GitHub storage and limited memory usage (DuckDB).\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set input and output directories\n",
    "input_dir = Path(\"data/raw_csvs\")\n",
    "output_dir = Path(\"data\")\n",
    "\n",
    "# Make sure the output directory exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loop through all CSV files in the input directory\n",
    "for csv_file in input_dir.glob(\"*.csv\"):\n",
    "    print(f\"Converting {csv_file.name}...\")\n",
    "\n",
    "    # Read CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create output path by replacing .csv with .parquet\n",
    "    parquet_file = output_dir / csv_file.with_suffix(\".parquet\").name\n",
    "\n",
    "    # Write to Parquet\n",
    "    df.to_parquet(parquet_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Saved to {parquet_file}\")\n",
    "\n",
    "     # Delete original CSV\n",
    "    csv_file.unlink()\n",
    "    print(f\"Deleted original CSV: {csv_file.name}\")\n",
    "\n",
    "print(\"✅ CSV to Parquet conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting max month...\n",
      "Running asset-level query and writing to parquet file, this may take a while...\n",
      "✅ Asset parquet file exported\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code updates the asset data\n",
    "\n",
    "'''\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/asset_emissions/country_subsector_level/asset_emissions_country_subsector.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "print(\"Getting max month...\")\n",
    "max_date = con.execute(f\"\"\"\n",
    "    select max(start_time)\n",
    "    from postgres_scan('{postgres_url}', 'public', 'asset_emissions')                       \n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(\"Running asset-level query and writing to parquet file, this may take a while...\")\n",
    "con.execute(f\"\"\"\n",
    "    INSTALL postgres;\n",
    "    LOAD postgres;\n",
    "\n",
    "    CREATE TABLE asset_emissions_parquet AS\n",
    "    SELECT ae.iso3_country,\n",
    "        ae.original_inventory_sector,\n",
    "        ae.start_time,\n",
    "        ae.gas,\n",
    "        sch.sector,\n",
    "        ca.name as country_name,\n",
    "        ca.continent,\n",
    "        ca.unfccc_annex,\n",
    "        ca.em_finance,\n",
    "        ca.eu,\n",
    "        ca.oecd,\n",
    "        ca.developed_un,\n",
    "        ae.release,\n",
    "        sum(emissions_quantity) emissions_quantity,\n",
    "        sum(activity) activity,\n",
    "        sum(emissions_quantity) / sum(activity) weighted_average_emissions_factor\n",
    "    \n",
    "    FROM postgres_scan('{postgres_url}', 'public', 'asset_emissions') ae\n",
    "    LEFT JOIN postgres_scan('{postgres_url}', 'public', 'country_analysis') ca\n",
    "        ON CAST(ca.iso3_country AS VARCHAR) = CAST(ae.iso3_country AS VARCHAR)\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT sector, subsector FROM postgres_scan('{postgres_url}', 'public', 'asset_schema')\n",
    "    ) sch\n",
    "        ON CAST(sch.subsector AS VARCHAR) = CAST(ae.original_inventory_sector AS VARCHAR)\n",
    "    \n",
    "    WHERE ae.start_time >= (\n",
    "                date_trunc('year', DATE '{max_date}') - INTERVAL '3 YEARS'\n",
    "            )\n",
    "      AND ae.gas in ('co2e_100yr','ch4')\n",
    "      AND ae.most_granular = TRUE\n",
    "    \n",
    "    GROUP BY ae.iso3_country,\n",
    "        ae.original_inventory_sector,\n",
    "        ae.start_time,\n",
    "        ae.gas,\n",
    "        sch.sector,\n",
    "        ca.name,\n",
    "        ca.continent,\n",
    "        ca.unfccc_annex,\n",
    "        ca.em_finance,\n",
    "        ca.eu,\n",
    "        ca.oecd,\n",
    "        ca.developed_un,\n",
    "        ae.release;\n",
    "\n",
    "    COPY asset_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "con.close()\n",
    "\n",
    "print(\"✅ Asset parquet file exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f6c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ Asset Annual Emissions ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/landing_zone/asset_annual_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "print('Running query...')\n",
    "con.execute( f'''\n",
    "\tINSTALL postgres;\n",
    "\tLOAD postgres;\n",
    "\n",
    "\tCREATE TABLE asset_annual_emissions_parquet AS\n",
    "\tselect extract(year from ae.start_time) as year\n",
    "\t\t, ae.asset_id\n",
    "\t\t, ai.asset_type\n",
    "\t\t, CASE \n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'iron-and-steel' AND ai.asset_type LIKE '%BF%' \n",
    "\t\t\t\t\tTHEN '{{''iron-and-steel'': [''BF'', ''DRI-EAF'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Refinery%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Refinery'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Smelting%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Smelting'']}}'\n",
    "\t\t\t\tELSE 'all' \n",
    "\t\t\tEND AS asset_type_2\n",
    "\t\t, ai.asset_name\n",
    "\t\t, ae.iso3_country\n",
    "\t\t, ca.name as country_name\n",
    "        , abc.region balancing_authority_region\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ae.original_inventory_sector as subsector\n",
    "\t\t, al.gadm_1\n",
    "\t\t, al.gadm_2\n",
    "\t\t, al.ghs_fua\n",
    "\t\t, al.city_id\n",
    "\t\t, ae.other1\n",
    "\t\t, ae.other2\n",
    "\t\t, ae.other3\n",
    "\t\t, ae.other4\n",
    "\t\t, ae.other5\n",
    "\t\t, ae.other6\n",
    "\t\t, ae.other7\n",
    "\t\t, ae.other8\n",
    "\t\t, ae.other9\n",
    "\t\t, ae.other10\n",
    "\t\t, ae.activity_units\n",
    "\t\t, sum(capacity) capacity\n",
    "\t\t, sum(activity) activity\n",
    "\t\t, avg(emissions_factor) average_emissions_factor\n",
    "\t\t, sum(emissions_quantity) emissions_quantity\n",
    "        , ers.strategy_id\n",
    "\t\t, ers.strategy_name\n",
    "\t\t, ers.strategy_description\n",
    "\t\t, ers.mechanism\n",
    "\t\t, ers.old_activity\n",
    "\t\t, ers.affected_activity\n",
    "\t\t, ers.old_emissions_factor\n",
    "\t\t, ers.new_emissions_factor\n",
    "\t\t, ers.emissions_reduced_at_asset\n",
    "\t\t, ers.induced_sector_1\n",
    "\t\t, ers.induced_sector_1_induced_emissions\n",
    "\t\t, ers.induced_sector_2\n",
    "\t\t, ers.induced_sector_2_induced_emissions\n",
    "\t\t, ers.induced_sector_3\n",
    "\t\t, ers.induced_sector_3_induced_emissions\n",
    "\t\t, ers.total_emissions_reduced_per_year\n",
    "\n",
    "\tfrom postgres_scan('{postgres_url}','public', 'asset_emissions') ae\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'asset_information') ai\n",
    "\t\ton ai.asset_id = ae.asset_id\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'asset_location') al\n",
    "\t\ton al.asset_id = ae.asset_id\n",
    "\tleft join (\n",
    "\t\tselect distinct sector, subsector from postgres_scan('{postgres_url}','public', 'asset_schema')\n",
    "\t) asch\n",
    "\t\ton cast(asch.subsector as varchar) = cast(ae.original_inventory_sector as varchar)\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(ae.iso3_country as varchar)\n",
    "    left join postgres_scan('{postgres_url}','public', 'asset_ba_crosswalk') abc\n",
    "\t\ton cast(abc.asset_id as text) = cast(ae.asset_id as text)\n",
    "    left join (\n",
    "\t\tselect rdf.* \n",
    "\t\tfrom postgres_scan('{postgres_url}','public','reductions_data_fusion') rdf\n",
    "        where strategy_rank = 1\n",
    "\t\t\tand rdf.gas = 'co2e_100yr'\n",
    "    ) ers\n",
    "\t\ton ers.asset_id = ae.asset_id\n",
    "\n",
    "\twhere extract(year from ae.start_time) = 2024\n",
    "\t\tand ae.most_granular = true\n",
    "\t\tand ae.gas = 'co2e_100yr'\n",
    "\t\tand ae.original_inventory_sector not in ('forest-land-clearing',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-degradation',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'net-forest-land',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'net-shrubgrass',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'net-wetland',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'removals',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'shrubgrass-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'water-reservoirs',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'wetland-fires')\n",
    "\n",
    "\tgroup by extract(year from ae.start_time)\n",
    "\t\t, ae.asset_id\n",
    "\t\t, ai.asset_type\n",
    "        , CASE \n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'iron-and-steel' AND ai.asset_type LIKE '%BF%' \n",
    "\t\t\t\t\tTHEN '{{''iron-and-steel'': [''BF'', ''DRI-EAF'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Refinery%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Refinery'']}}'\n",
    "\t\t\t\tWHEN ae.original_inventory_sector = 'aluminum' AND ai.asset_type LIKE '%Smelting%' \n",
    "\t\t\t\t\tTHEN '{{''aluminum'': [''Smelting'']}}'\n",
    "\t\t\t\tELSE 'all' \n",
    "\t\t\tEND\n",
    "\t\t, ai.asset_name\n",
    "\t\t, ae.iso3_country\n",
    "\t\t, ca.name\n",
    "        , abc.region\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ae.original_inventory_sector\n",
    "\t\t, al.gadm_1\n",
    "\t\t, al.gadm_2\n",
    "\t\t, al.ghs_fua\n",
    "\t\t, al.city_id\n",
    "\t\t, ae.other1\n",
    "\t\t, ae.other2\n",
    "\t\t, ae.other3\n",
    "\t\t, ae.other4\n",
    "\t\t, ae.other5\n",
    "\t\t, ae.other6\n",
    "\t\t, ae.other7\n",
    "\t\t, ae.other8\n",
    "\t\t, ae.other9\n",
    "\t\t, ae.other10\n",
    "\t\t, ae.activity_units\n",
    "        , ers.strategy_id\n",
    "\t\t, ers.strategy_name\n",
    "\t\t, ers.strategy_description\n",
    "\t\t, ers.mechanism\n",
    "\t\t, ers.old_activity\n",
    "\t\t, ers.affected_activity\n",
    "\t\t, ers.old_emissions_factor\n",
    "\t\t, ers.new_emissions_factor\n",
    "\t\t, ers.emissions_reduced_at_asset\n",
    "\t\t, ers.induced_sector_1\n",
    "\t\t, ers.induced_sector_1_induced_emissions\n",
    "\t\t, ers.induced_sector_2\n",
    "\t\t, ers.induced_sector_2_induced_emissions\n",
    "\t\t, ers.induced_sector_3\n",
    "\t\t, ers.induced_sector_3_induced_emissions\n",
    "\t\t, ers.total_emissions_reduced_per_year;\n",
    "            \n",
    "    COPY asset_annual_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "            \n",
    "    ''')\n",
    "\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------- ADD MOER FACTORS --------------------------------------\n",
    "\n",
    "# import duckdb\n",
    "from utils.utils import data_add_moer\n",
    "import pandas as pd\n",
    "\n",
    "asset_parquet_path = 'data/landing_zone/asset_annual_emissions.parquet'\n",
    "output_path = 'data/landing_zone/asset_annual_emissions_moer.parquet'\n",
    "\n",
    "df_asset = pd.read_parquet(asset_parquet_path)\n",
    "\n",
    "asset_moer_df = data_add_moer(df_asset, cond={\"moer\": True})\n",
    "\n",
    "asset_moer_df.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e07028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/asset_annual_emissions/chunk_1.parquet (51.7 MB, rows 0–391822)\n",
      "Saved data/asset_annual_emissions/chunk_2.parquet (52.0 MB, rows 391822–783644)\n",
      "Saved data/asset_annual_emissions/chunk_3.parquet (51.9 MB, rows 783644–1175466)\n",
      "Saved data/asset_annual_emissions/chunk_4.parquet (52.0 MB, rows 1175466–1567288)\n",
      "Saved data/asset_annual_emissions/chunk_5.parquet (52.0 MB, rows 1567288–1959110)\n",
      "Saved data/asset_annual_emissions/chunk_6.parquet (51.7 MB, rows 1959110–2350932)\n",
      "Saved data/asset_annual_emissions/chunk_7.parquet (51.9 MB, rows 2350932–2742754)\n",
      "Saved data/asset_annual_emissions/chunk_8.parquet (52.0 MB, rows 2742754–3134576)\n",
      "Saved data/asset_annual_emissions/chunk_9.parquet (46.8 MB, rows 3134576–3484505)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ SPLITS LARGE ASSET FILE INTO ~50MB CHUNKS ---------------------------------\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "input_file = \"data/landing_zone/asset_annual_emissions_moer.parquet\"  # Your large file\n",
    "output_dir = \"data/asset_emissions/asset_level_2024\"  # Destination folder\n",
    "target_size_mb = 45  # keep each file safely under 100MB, avoid github warning\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load full Parquet into DataFrame\n",
    "df = pd.read_parquet(input_file)\n",
    "total_rows = len(df)\n",
    "\n",
    "# Estimate file size per row using a small sample\n",
    "test_sample = df.iloc[:10000]\n",
    "test_table = pa.Table.from_pandas(test_sample)\n",
    "pq.write_table(test_table, \"temp.parquet\")\n",
    "bytes_per_row = os.path.getsize(\"temp.parquet\") / len(test_sample)\n",
    "os.remove(\"temp.parquet\")\n",
    "\n",
    "# Determine number of rows per ~50MB chunk\n",
    "target_bytes = target_size_mb * 1024 * 1024\n",
    "rows_per_chunk = int(target_bytes / bytes_per_row)\n",
    "\n",
    "# Split and write files\n",
    "for i, start in enumerate(range(0, total_rows, rows_per_chunk)):\n",
    "    end = min(start + rows_per_chunk, total_rows)\n",
    "    chunk_df = df.iloc[start:end]\n",
    "    chunk_table = pa.Table.from_pandas(chunk_df)\n",
    "    output_path = os.path.join(output_dir, f\"chunk_{i+1}.parquet\")\n",
    "    pq.write_table(chunk_table, output_path)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"Saved {output_path} ({size_mb:.1f} MB, rows {start}–{end})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8defd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------ GADM 1 Emissions ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/gadm_emissions/gadm_1/gadm_1_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "print('Running query')\n",
    "con.execute(f'''\n",
    "    INSTALL postgres;\n",
    "    LOAD postgres;\n",
    "\n",
    "    CREATE TABLE gadm_1_emissions_parquet AS\n",
    "    select extract(year from g1e.start_time) as year \n",
    "        , g1e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g1e.iso3_country\n",
    "        , ca.name as country_name\n",
    "        , gb.name gadm_1_name\n",
    "        , gb.corrected_name gadm_1_corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g1e.original_inventory_sector subsector\n",
    "        , g1e.gas\n",
    "        , sum(asset_activity) asset_activity\n",
    "        , sum(asset_emissions) asset_emissions\n",
    "        , sum(remainder_activity) remainder_activity\n",
    "        , sum(remainder_emissions) remainder_emissions\n",
    "        , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "    from postgres_scan('{postgres_url}', 'public', 'gadm_1_emissions') g1e\n",
    "    inner join (\n",
    "        select distinct gadm_id\n",
    "            , gid\n",
    "            , name\n",
    "            , corrected_name\n",
    "            , admin_level\n",
    "        from postgres_scan('{postgres_url}','public', 'gadm_boundaries') \n",
    "        where admin_level = 1\n",
    "    ) as gb\n",
    "        on g1e.gadm_id = gb.gadm_id\n",
    "    left join (\n",
    "        select distinct sector\n",
    "            , subsector\n",
    "        from postgres_scan('{postgres_url}','public', 'asset_schema') \n",
    "    ) asch\n",
    "        on cast(asch.subsector as varchar) = cast(g1e.original_inventory_sector as varchar)\n",
    "    left join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(g1e.iso3_country as varchar)\n",
    "\n",
    "    where g1e.gas = 'co2e_100yr'\n",
    "        and extract(year from start_time) = 2024\n",
    "        and g1e.original_inventory_sector not in ('forest-land-clearing',\n",
    "                                                'forest-land-degradation',\n",
    "                                                'forest-land-fires',\n",
    "                                                'net-forest-land',\n",
    "                                                'net-shrubgrass',\n",
    "                                                'net-wetland',\n",
    "                                                'removals',\n",
    "                                                'shrubgrass-fires',\n",
    "                                                'water-reservoirs',\n",
    "                                                'wetland-fires')\n",
    "\n",
    "    group by extract(year from g1e.start_time) \n",
    "        , g1e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g1e.iso3_country\n",
    "        , ca.name\n",
    "        , gb.name \n",
    "        , gb.corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g1e.original_inventory_sector\n",
    "        , g1e.gas;\n",
    "\n",
    "    COPY gadm_1_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "''')\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04194ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing gadm_2 query...\n",
      "Processed batch 1 (10000 rows), total rows: 10000\n",
      "Processed batch 2 (10000 rows), total rows: 20000\n",
      "Processed batch 3 (10000 rows), total rows: 30000\n",
      "Processed batch 4 (10000 rows), total rows: 40000\n",
      "Processed batch 5 (10000 rows), total rows: 50000\n",
      "Processed batch 6 (10000 rows), total rows: 60000\n",
      "Processed batch 7 (10000 rows), total rows: 70000\n",
      "Processed batch 8 (10000 rows), total rows: 80000\n",
      "Processed batch 9 (10000 rows), total rows: 90000\n",
      "Processed batch 10 (10000 rows), total rows: 100000\n",
      "Processed batch 11 (10000 rows), total rows: 110000\n",
      "Processed batch 12 (10000 rows), total rows: 120000\n",
      "Processed batch 13 (10000 rows), total rows: 130000\n",
      "Processed batch 14 (10000 rows), total rows: 140000\n",
      "Processed batch 15 (10000 rows), total rows: 150000\n",
      "Processed batch 16 (10000 rows), total rows: 160000\n",
      "Processed batch 17 (10000 rows), total rows: 170000\n",
      "Processed batch 18 (10000 rows), total rows: 180000\n",
      "Processed batch 19 (10000 rows), total rows: 190000\n",
      "Processed batch 20 (10000 rows), total rows: 200000\n",
      "Processed batch 21 (10000 rows), total rows: 210000\n",
      "Processed batch 22 (10000 rows), total rows: 220000\n",
      "Processed batch 23 (10000 rows), total rows: 230000\n",
      "Processed batch 24 (10000 rows), total rows: 240000\n",
      "Processed batch 25 (10000 rows), total rows: 250000\n",
      "Processed batch 26 (10000 rows), total rows: 260000\n",
      "Processed batch 27 (10000 rows), total rows: 270000\n",
      "Processed batch 28 (10000 rows), total rows: 280000\n",
      "Processed batch 29 (10000 rows), total rows: 290000\n",
      "Processed batch 30 (10000 rows), total rows: 300000\n",
      "Processed batch 31 (10000 rows), total rows: 310000\n",
      "Processed batch 32 (10000 rows), total rows: 320000\n",
      "Processed batch 33 (10000 rows), total rows: 330000\n",
      "Processed batch 34 (10000 rows), total rows: 340000\n",
      "Processed batch 35 (10000 rows), total rows: 350000\n",
      "Processed batch 36 (10000 rows), total rows: 360000\n",
      "Processed batch 37 (10000 rows), total rows: 370000\n",
      "Processed batch 38 (10000 rows), total rows: 380000\n",
      "Processed batch 39 (10000 rows), total rows: 390000\n",
      "Processed batch 40 (10000 rows), total rows: 400000\n",
      "Processed batch 41 (10000 rows), total rows: 410000\n",
      "Processed batch 42 (10000 rows), total rows: 420000\n",
      "Processed batch 43 (10000 rows), total rows: 430000\n",
      "Processed batch 44 (10000 rows), total rows: 440000\n",
      "Processed batch 45 (10000 rows), total rows: 450000\n",
      "Processed batch 46 (10000 rows), total rows: 460000\n",
      "Processed batch 47 (10000 rows), total rows: 470000\n",
      "Processed batch 48 (10000 rows), total rows: 480000\n",
      "Processed batch 49 (10000 rows), total rows: 490000\n",
      "Processed batch 50 (10000 rows), total rows: 500000\n",
      "Processed batch 51 (10000 rows), total rows: 510000\n",
      "Processed batch 52 (10000 rows), total rows: 520000\n",
      "Processed batch 53 (10000 rows), total rows: 530000\n",
      "Processed batch 54 (10000 rows), total rows: 540000\n",
      "Processed batch 55 (10000 rows), total rows: 550000\n",
      "Processed batch 56 (10000 rows), total rows: 560000\n",
      "Processed batch 57 (10000 rows), total rows: 570000\n",
      "Processed batch 58 (10000 rows), total rows: 580000\n",
      "Processed batch 59 (10000 rows), total rows: 590000\n",
      "Processed batch 60 (10000 rows), total rows: 600000\n",
      "Processed batch 61 (10000 rows), total rows: 610000\n",
      "Processed batch 62 (10000 rows), total rows: 620000\n",
      "Processed batch 63 (10000 rows), total rows: 630000\n",
      "Processed batch 64 (10000 rows), total rows: 640000\n",
      "Processed batch 65 (10000 rows), total rows: 650000\n",
      "Processed batch 66 (10000 rows), total rows: 660000\n",
      "Processed batch 67 (10000 rows), total rows: 670000\n",
      "Processed batch 68 (10000 rows), total rows: 680000\n",
      "Processed batch 69 (10000 rows), total rows: 690000\n",
      "Processed batch 70 (10000 rows), total rows: 700000\n",
      "Processed batch 71 (10000 rows), total rows: 710000\n",
      "Processed batch 72 (10000 rows), total rows: 720000\n",
      "Processed batch 73 (10000 rows), total rows: 730000\n",
      "Processed batch 74 (10000 rows), total rows: 740000\n",
      "Processed batch 75 (10000 rows), total rows: 750000\n",
      "Processed batch 76 (10000 rows), total rows: 760000\n",
      "Processed batch 77 (10000 rows), total rows: 770000\n",
      "Processed batch 78 (10000 rows), total rows: 780000\n",
      "Processed batch 79 (10000 rows), total rows: 790000\n",
      "Processed batch 80 (10000 rows), total rows: 800000\n",
      "Processed batch 81 (10000 rows), total rows: 810000\n",
      "Processed batch 82 (10000 rows), total rows: 820000\n",
      "Processed batch 83 (10000 rows), total rows: 830000\n",
      "Processed batch 84 (10000 rows), total rows: 840000\n",
      "Processed batch 85 (10000 rows), total rows: 850000\n",
      "Processed batch 86 (10000 rows), total rows: 860000\n",
      "Processed batch 87 (10000 rows), total rows: 870000\n",
      "Processed batch 88 (10000 rows), total rows: 880000\n",
      "Processed batch 89 (10000 rows), total rows: 890000\n",
      "Processed batch 90 (10000 rows), total rows: 900000\n",
      "Processed batch 91 (10000 rows), total rows: 910000\n",
      "Processed batch 92 (10000 rows), total rows: 920000\n",
      "Processed batch 93 (10000 rows), total rows: 930000\n",
      "Processed batch 94 (10000 rows), total rows: 940000\n",
      "Processed batch 95 (10000 rows), total rows: 950000\n",
      "Processed batch 96 (10000 rows), total rows: 960000\n",
      "Processed batch 97 (10000 rows), total rows: 970000\n",
      "Processed batch 98 (10000 rows), total rows: 980000\n",
      "Processed batch 99 (10000 rows), total rows: 990000\n",
      "Processed batch 100 (10000 rows), total rows: 1000000\n",
      "Processed batch 101 (10000 rows), total rows: 1010000\n",
      "Processed batch 102 (10000 rows), total rows: 1020000\n",
      "Processed batch 103 (10000 rows), total rows: 1030000\n",
      "Processed batch 104 (10000 rows), total rows: 1040000\n",
      "Processed batch 105 (10000 rows), total rows: 1050000\n",
      "Processed batch 106 (10000 rows), total rows: 1060000\n",
      "Processed batch 107 (10000 rows), total rows: 1070000\n",
      "Processed batch 108 (10000 rows), total rows: 1080000\n",
      "Processed batch 109 (10000 rows), total rows: 1090000\n",
      "Processed batch 110 (10000 rows), total rows: 1100000\n",
      "Processed batch 111 (10000 rows), total rows: 1110000\n",
      "Processed batch 112 (10000 rows), total rows: 1120000\n",
      "Processed batch 113 (10000 rows), total rows: 1130000\n",
      "Processed batch 114 (10000 rows), total rows: 1140000\n",
      "Processed batch 115 (10000 rows), total rows: 1150000\n",
      "Processed batch 116 (10000 rows), total rows: 1160000\n",
      "Processed batch 117 (10000 rows), total rows: 1170000\n",
      "Processed batch 118 (10000 rows), total rows: 1180000\n",
      "Processed batch 119 (10000 rows), total rows: 1190000\n",
      "Processed batch 120 (10000 rows), total rows: 1200000\n",
      "Processed batch 121 (10000 rows), total rows: 1210000\n",
      "Processed batch 122 (10000 rows), total rows: 1220000\n",
      "Processed batch 123 (10000 rows), total rows: 1230000\n",
      "Processed batch 124 (10000 rows), total rows: 1240000\n",
      "Processed batch 125 (10000 rows), total rows: 1250000\n",
      "Processed batch 126 (10000 rows), total rows: 1260000\n",
      "Processed batch 127 (10000 rows), total rows: 1270000\n",
      "Processed batch 128 (10000 rows), total rows: 1280000\n",
      "Processed batch 129 (10000 rows), total rows: 1290000\n",
      "Processed batch 130 (10000 rows), total rows: 1300000\n",
      "Processed batch 131 (10000 rows), total rows: 1310000\n",
      "Processed batch 132 (10000 rows), total rows: 1320000\n",
      "Processed batch 133 (10000 rows), total rows: 1330000\n",
      "Processed batch 134 (10000 rows), total rows: 1340000\n",
      "Processed batch 135 (10000 rows), total rows: 1350000\n",
      "Processed batch 136 (10000 rows), total rows: 1360000\n",
      "Processed batch 137 (10000 rows), total rows: 1370000\n",
      "Processed batch 138 (10000 rows), total rows: 1380000\n",
      "Processed batch 139 (10000 rows), total rows: 1390000\n",
      "Processed batch 140 (10000 rows), total rows: 1400000\n",
      "Processed batch 141 (10000 rows), total rows: 1410000\n",
      "Processed batch 142 (10000 rows), total rows: 1420000\n",
      "Processed batch 143 (10000 rows), total rows: 1430000\n",
      "Processed batch 144 (10000 rows), total rows: 1440000\n",
      "Processed batch 145 (10000 rows), total rows: 1450000\n",
      "Processed batch 146 (10000 rows), total rows: 1460000\n",
      "Processed batch 147 (10000 rows), total rows: 1470000\n",
      "Processed batch 148 (10000 rows), total rows: 1480000\n",
      "Processed batch 149 (10000 rows), total rows: 1490000\n",
      "Processed batch 150 (10000 rows), total rows: 1500000\n",
      "Processed batch 151 (10000 rows), total rows: 1510000\n",
      "Processed batch 152 (10000 rows), total rows: 1520000\n",
      "Processed batch 153 (10000 rows), total rows: 1530000\n",
      "Processed batch 154 (10000 rows), total rows: 1540000\n",
      "Processed batch 155 (10000 rows), total rows: 1550000\n",
      "Processed batch 156 (10000 rows), total rows: 1560000\n",
      "Processed batch 157 (10000 rows), total rows: 1570000\n",
      "Processed batch 158 (10000 rows), total rows: 1580000\n",
      "Processed batch 159 (10000 rows), total rows: 1590000\n",
      "Processed batch 160 (10000 rows), total rows: 1600000\n",
      "Processed batch 161 (10000 rows), total rows: 1610000\n",
      "Processed batch 162 (10000 rows), total rows: 1620000\n",
      "Processed batch 163 (10000 rows), total rows: 1630000\n",
      "Processed batch 164 (10000 rows), total rows: 1640000\n",
      "Processed batch 165 (10000 rows), total rows: 1650000\n",
      "Processed batch 166 (10000 rows), total rows: 1660000\n",
      "Processed batch 167 (10000 rows), total rows: 1670000\n",
      "Processed batch 168 (10000 rows), total rows: 1680000\n",
      "Processed batch 169 (10000 rows), total rows: 1690000\n",
      "Processed batch 170 (10000 rows), total rows: 1700000\n",
      "Processed batch 171 (10000 rows), total rows: 1710000\n",
      "Processed batch 172 (10000 rows), total rows: 1720000\n",
      "Processed batch 173 (10000 rows), total rows: 1730000\n",
      "Processed batch 174 (10000 rows), total rows: 1740000\n",
      "Processed batch 175 (10000 rows), total rows: 1750000\n",
      "Processed batch 176 (10000 rows), total rows: 1760000\n",
      "Processed batch 177 (10000 rows), total rows: 1770000\n",
      "Processed batch 178 (10000 rows), total rows: 1780000\n",
      "Processed batch 179 (10000 rows), total rows: 1790000\n",
      "Processed batch 180 (10000 rows), total rows: 1800000\n",
      "Processed batch 181 (10000 rows), total rows: 1810000\n",
      "Processed batch 182 (10000 rows), total rows: 1820000\n",
      "Processed batch 183 (10000 rows), total rows: 1830000\n",
      "Processed batch 184 (10000 rows), total rows: 1840000\n",
      "Processed batch 185 (10000 rows), total rows: 1850000\n",
      "Processed batch 186 (10000 rows), total rows: 1860000\n",
      "Processed batch 187 (10000 rows), total rows: 1870000\n",
      "Processed batch 188 (10000 rows), total rows: 1880000\n",
      "Processed batch 189 (10000 rows), total rows: 1890000\n",
      "Processed batch 190 (10000 rows), total rows: 1900000\n",
      "Processed batch 191 (10000 rows), total rows: 1910000\n",
      "Processed batch 192 (10000 rows), total rows: 1920000\n",
      "Processed batch 193 (10000 rows), total rows: 1930000\n",
      "Processed batch 194 (10000 rows), total rows: 1940000\n",
      "Processed batch 195 (10000 rows), total rows: 1950000\n",
      "Processed batch 196 (10000 rows), total rows: 1960000\n",
      "Processed batch 197 (10000 rows), total rows: 1970000\n",
      "Processed batch 198 (10000 rows), total rows: 1980000\n",
      "Processed batch 199 (10000 rows), total rows: 1990000\n",
      "Processed batch 200 (10000 rows), total rows: 2000000\n",
      "Processed batch 201 (10000 rows), total rows: 2010000\n",
      "Processed batch 202 (10000 rows), total rows: 2020000\n",
      "Processed batch 203 (10000 rows), total rows: 2030000\n",
      "Processed batch 204 (10000 rows), total rows: 2040000\n",
      "Processed batch 205 (10000 rows), total rows: 2050000\n",
      "Processed batch 206 (10000 rows), total rows: 2060000\n",
      "Processed batch 207 (10000 rows), total rows: 2070000\n",
      "Processed batch 208 (10000 rows), total rows: 2080000\n",
      "Processed batch 209 (10000 rows), total rows: 2090000\n",
      "Processed batch 210 (10000 rows), total rows: 2100000\n",
      "Processed batch 211 (10000 rows), total rows: 2110000\n",
      "Processed batch 212 (10000 rows), total rows: 2120000\n",
      "Processed batch 213 (10000 rows), total rows: 2130000\n",
      "Processed batch 214 (10000 rows), total rows: 2140000\n",
      "Processed batch 215 (10000 rows), total rows: 2150000\n",
      "Processed batch 216 (10000 rows), total rows: 2160000\n",
      "Processed batch 217 (10000 rows), total rows: 2170000\n",
      "Processed batch 218 (10000 rows), total rows: 2180000\n",
      "Processed batch 219 (10000 rows), total rows: 2190000\n",
      "Processed batch 220 (10000 rows), total rows: 2200000\n",
      "Processed batch 221 (10000 rows), total rows: 2210000\n",
      "Processed batch 222 (10000 rows), total rows: 2220000\n",
      "Processed batch 223 (10000 rows), total rows: 2230000\n",
      "Processed batch 224 (10000 rows), total rows: 2240000\n",
      "Processed batch 225 (10000 rows), total rows: 2250000\n",
      "Processed batch 226 (10000 rows), total rows: 2260000\n",
      "Processed batch 227 (10000 rows), total rows: 2270000\n",
      "Processed batch 228 (10000 rows), total rows: 2280000\n",
      "Processed batch 229 (10000 rows), total rows: 2290000\n",
      "Processed batch 230 (10000 rows), total rows: 2300000\n",
      "Processed batch 231 (10000 rows), total rows: 2310000\n",
      "Processed batch 232 (10000 rows), total rows: 2320000\n",
      "Processed batch 233 (10000 rows), total rows: 2330000\n",
      "Processed batch 234 (10000 rows), total rows: 2340000\n",
      "Processed batch 235 (10000 rows), total rows: 2350000\n",
      "Processed batch 236 (10000 rows), total rows: 2360000\n",
      "Processed batch 237 (10000 rows), total rows: 2370000\n",
      "Processed batch 238 (10000 rows), total rows: 2380000\n",
      "Processed batch 239 (10000 rows), total rows: 2390000\n",
      "Processed batch 240 (10000 rows), total rows: 2400000\n",
      "Processed batch 241 (10000 rows), total rows: 2410000\n",
      "Processed batch 242 (10000 rows), total rows: 2420000\n",
      "Processed batch 243 (10000 rows), total rows: 2430000\n",
      "Processed batch 244 (10000 rows), total rows: 2440000\n",
      "Processed batch 245 (10000 rows), total rows: 2450000\n",
      "Processed batch 246 (10000 rows), total rows: 2460000\n",
      "Processed batch 247 (10000 rows), total rows: 2470000\n",
      "Processed batch 248 (10000 rows), total rows: 2480000\n",
      "Processed batch 249 (10000 rows), total rows: 2490000\n",
      "Processed batch 250 (10000 rows), total rows: 2500000\n",
      "Processed batch 251 (10000 rows), total rows: 2510000\n",
      "Processed batch 252 (10000 rows), total rows: 2520000\n",
      "Processed batch 253 (10000 rows), total rows: 2530000\n",
      "Processed batch 254 (10000 rows), total rows: 2540000\n",
      "Processed batch 255 (10000 rows), total rows: 2550000\n",
      "Processed batch 256 (10000 rows), total rows: 2560000\n",
      "Processed batch 257 (10000 rows), total rows: 2570000\n",
      "Processed batch 258 (10000 rows), total rows: 2580000\n",
      "Processed batch 259 (10000 rows), total rows: 2590000\n",
      "Processed batch 260 (10000 rows), total rows: 2600000\n",
      "Processed batch 261 (10000 rows), total rows: 2610000\n",
      "Processed batch 262 (10000 rows), total rows: 2620000\n",
      "Processed batch 263 (10000 rows), total rows: 2630000\n",
      "Processed batch 264 (10000 rows), total rows: 2640000\n",
      "Processed batch 265 (10000 rows), total rows: 2650000\n",
      "Processed batch 266 (10000 rows), total rows: 2660000\n",
      "Processed batch 267 (10000 rows), total rows: 2670000\n",
      "Processed batch 268 (10000 rows), total rows: 2680000\n",
      "Processed batch 269 (10000 rows), total rows: 2690000\n",
      "Processed batch 270 (10000 rows), total rows: 2700000\n",
      "Processed batch 271 (10000 rows), total rows: 2710000\n",
      "Processed batch 272 (10000 rows), total rows: 2720000\n",
      "Processed batch 273 (10000 rows), total rows: 2730000\n",
      "Processed batch 274 (8586 rows), total rows: 2738586\n",
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------- GADM 2 BATCH -----------------------------------------------------------------\n",
    "\n",
    "import psycopg2\n",
    "from urllib.parse import quote_plus\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import csv\n",
    "import os\n",
    "\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=database,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    host=host,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "cur = conn.cursor(name='parquet_cursor')  # server-side cursor\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "     select extract(year from ge.start_time) as year \n",
    "        , gb1.gadm_id gadm_1_id\n",
    "        , gb1.name gadm_1_name\n",
    "        , gb1.corrected_name gadm_1_corrected_name\n",
    "        , ge.gadm_id gadm_2_id\n",
    "        , gb2.name gadm_2_name\n",
    "        , gb2.corrected_name gadm_2_corrected_name\n",
    "        , gb2.gid\n",
    "        , gb2.admin_level\n",
    "        , ge.iso3_country\n",
    "        , ca.name as country_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , ge.original_inventory_sector subsector\n",
    "        , sum(asset_activity) asset_activity\n",
    "        , sum(asset_emissions) asset_emissions\n",
    "        , sum(remainder_activity) remainder_activity\n",
    "        , sum(remainder_emissions) remainder_emissions\n",
    "        , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "    from gadm_emissions ge\n",
    "    inner join (\n",
    "        select distinct gadm_id\n",
    "            , gid\n",
    "            , immediate_parent\n",
    "            , name\n",
    "            , corrected_name\n",
    "            , admin_level\n",
    "        from gadm_boundaries\n",
    "        where admin_level = 2\n",
    "    ) as gb2\n",
    "        on ge.gadm_id = gb2.gadm_id\n",
    "    left join (\n",
    "        select distinct sector\n",
    "            , subsector\n",
    "        from asset_schema\n",
    "    ) asch\n",
    "        on cast(asch.subsector as varchar) = cast(ge.original_inventory_sector as varchar)\n",
    "    left join (\n",
    "        select gadm_id\n",
    "            , name\n",
    "            , corrected_name\n",
    "        from gadm_boundaries\n",
    "        where admin_level = 1\n",
    "    ) gb1\n",
    "        on gb1.gadm_id = gb2.immediate_parent\n",
    "    left join country_analysis ca\n",
    "        on cast(ca.iso3_country as varchar) = cast(ge.iso3_country as varchar)\n",
    "\n",
    "    where ge.gas = 'co2e_100yr'\n",
    "        and extract(year from start_time) = 2024\n",
    "        and ge.original_inventory_sector not in ('forest-land-clearing',\n",
    "                                                'forest-land-degradation',\n",
    "                                                'forest-land-fires',\n",
    "                                                'net-forest-land',\n",
    "                                                'net-shrubgrass',\n",
    "                                                'net-wetland',\n",
    "                                                'removals',\n",
    "                                                'shrubgrass-fires',\n",
    "                                                'water-reservoirs',\n",
    "                                                'wetland-fires')\n",
    "\n",
    "    group by extract(year from ge.start_time)\n",
    "        , gb1.gadm_id \n",
    "        , gb1.name\n",
    "        , gb1.corrected_name\n",
    "        , ge.gadm_id \n",
    "        , gb2.name\n",
    "        , gb2.corrected_name\n",
    "        , gb2.gid\n",
    "        , gb2.admin_level\n",
    "        , ge.iso3_country\n",
    "        , ca.name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , ge.original_inventory_sector\n",
    "    \"\"\")\n",
    "\n",
    "# Set up Parquet writer\n",
    "batch_size = 10000\n",
    "output_file = \"data/gadm_emissions/gadm_2/gadm_2_emissions.parquet\"\n",
    "batch_count = 0\n",
    "total_rows = 0\n",
    "\n",
    "print(\"executing gadm_2 query...\")\n",
    "\n",
    "# Fetch first batch\n",
    "rows = cur.fetchmany(batch_size)\n",
    "if not rows:\n",
    "    raise Exception(\"No data returned from query.\")\n",
    "\n",
    "field_names = [desc[0] for desc in cur.description]\n",
    "first_table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "writer = pq.ParquetWriter(output_file, first_table.schema)\n",
    "writer.write_table(first_table)\n",
    "batch_count += 1\n",
    "total_rows += len(rows)\n",
    "print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "# Process remaining batches\n",
    "while True:\n",
    "    rows = cur.fetchmany(batch_size)\n",
    "    if not rows:\n",
    "        break\n",
    "\n",
    "    table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "    table = table.cast(writer.schema)  # ensure schema matches first batch\n",
    "    writer.write_table(table)\n",
    "\n",
    "    batch_count += 1\n",
    "    total_rows += len(rows)\n",
    "    print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "writer.close()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ GADM_0 Emissions ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/gadm_emissions/gadm_0/gadm_0_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "print('Running query')\n",
    "con.execute(f'''\n",
    "    INSTALL postgres;\n",
    "    LOAD postgres;\n",
    "\n",
    "    CREATE TABLE gadm_0_emissions_parquet AS\n",
    "    select extract(year from g0e.start_time) as year \n",
    "        , g0e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g0e.iso3_country\n",
    "        , ca.name as country_name\n",
    "        , gb.name gadm_0_name\n",
    "        , gb.corrected_name gadm_0_corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g0e.original_inventory_sector subsector\n",
    "        , g0e.gas\n",
    "        , sum(asset_activity) asset_activity\n",
    "        , sum(asset_emissions) asset_emissions\n",
    "        , sum(remainder_activity) remainder_activity\n",
    "        , sum(remainder_emissions) remainder_emissions\n",
    "        , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "    from postgres_scan('{postgres_url}', 'public', 'gadm_0_emissions') g0e\n",
    "    inner join (\n",
    "        select distinct gadm_id\n",
    "            , gid\n",
    "            , name\n",
    "            , corrected_name\n",
    "            , admin_level\n",
    "        from postgres_scan('{postgres_url}','public', 'gadm_boundaries') \n",
    "        where admin_level = 0\n",
    "    ) as gb\n",
    "        on g0e.gadm_id = gb.gadm_id\n",
    "    left join (\n",
    "        select distinct sector\n",
    "            , subsector\n",
    "        from postgres_scan('{postgres_url}','public', 'asset_schema') \n",
    "    ) asch\n",
    "        on cast(asch.subsector as varchar) = cast(g0e.original_inventory_sector as varchar)\n",
    "    left join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(g0e.iso3_country as varchar)\n",
    "\n",
    "    where g0e.gas = 'co2e_100yr'\n",
    "        and extract(year from start_time) = 2024\n",
    "        and g0e.original_inventory_sector not in ('forest-land-clearing',\n",
    "                                                'forest-land-degradation',\n",
    "                                                'forest-land-fires',\n",
    "                                                'net-forest-land',\n",
    "                                                'net-shrubgrass',\n",
    "                                                'net-wetland',\n",
    "                                                'removals',\n",
    "                                                'shrubgrass-fires',\n",
    "                                                'water-reservoirs',\n",
    "                                                'wetland-fires')\n",
    "\n",
    "    group by extract(year from g0e.start_time) \n",
    "        , g0e.gadm_id\n",
    "        , gb.gid\n",
    "        , gb.admin_level\n",
    "        , g0e.iso3_country\n",
    "        , ca.name\n",
    "        , gb.name \n",
    "        , gb.corrected_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "        , asch.sector\n",
    "        , g0e.original_inventory_sector\n",
    "        , g0e.gas;\n",
    "\n",
    "    COPY gadm_0_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "''')\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ City Emissions ------------------------------------\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/city_emissions/city_emissions.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "print('Running query...')\n",
    "con.execute( f'''\n",
    "\tINSTALL postgres;\n",
    "\tLOAD postgres;\n",
    "\n",
    "\tCREATE TABLE city_emissions_parquet AS\n",
    "    \n",
    "\tselect extract(year from start_time) as year\n",
    "\t\t, ce.city_id\n",
    "\t\t, cb.name as city_name\n",
    "\t\t, cb.corrected_name as corrected_name\n",
    "\t\t, ce.iso3_country\n",
    "\t\t, ca.name as country_name\n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ce.original_inventory_sector as subsector\n",
    "\t\t, sum(asset_activity) asset_activity\n",
    "\t\t, sum(asset_emissions) asset_emissions\n",
    "\t\t, sum(remainder_activity) remainder_activity\n",
    "\t\t, sum(remainder_emissions) remainder_emissions\n",
    "\t\t, sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "\tfrom postgres_scan('{postgres_url}','public', 'city_emissions') ce\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'city_boundaries') cb\n",
    "\t\ton cb.city_id = ce.city_id\n",
    "        and cb.reporting_entity = 'ghs-fua'\n",
    "\tleft join (\n",
    "\t\tselect distinct sector, subsector\n",
    "\t\tfrom postgres_scan('{postgres_url}','public', 'asset_schema')\n",
    "\t) asch\n",
    "\t\ton cast(asch.subsector as varchar) = cast(ce.original_inventory_sector as varchar)\n",
    "\tleft join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "\t\ton cast(ca.iso3_country as varchar) = cast(ce.iso3_country as varchar)\n",
    "\n",
    "\twhere extract(year from ce.start_time) = 2024\n",
    "\t\tand ce.gas = 'co2e_100yr'\n",
    "\t\tand ce.original_inventory_sector not in ('forest-land-clearing',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-degradation',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'forest-land-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'net-forest-land',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'net-shrubgrass',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'net-wetland',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'removals',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'shrubgrass-fires',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'water-reservoirs',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'wetland-fires')\n",
    "\n",
    "\tgroup by extract(year from start_time) \n",
    "\t\t, ce.city_id\n",
    "\t\t, cb.name \n",
    "\t\t, cb.corrected_name \n",
    "\t\t, ce.iso3_country\n",
    "\t\t, ca.name \n",
    "        , ca.continent\n",
    "        , ca.eu\n",
    "        , ca.oecd\n",
    "        , ca.unfccc_annex\n",
    "        , ca.developed_un\n",
    "        , ca.em_finance\n",
    "\t\t, asch.sector\n",
    "\t\t, ce.original_inventory_sector;\n",
    "            \n",
    "    COPY city_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "            \n",
    "    ''')\n",
    "\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eb613882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ Asset Ownership ------------------------------------\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Build SQLAlchemy engine for PostgreSQL\n",
    "user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "parquet_path = \"data/ownership/asset_ownership.parquet\"\n",
    "\n",
    "# Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "con = duckdb.connect()\n",
    "\n",
    "print('Running query...')\n",
    "con.execute( f'''\n",
    "\tINSTALL postgres;\n",
    "\tLOAD postgres;\n",
    "\n",
    "\tCREATE TABLE asset_ownership_parquet AS\n",
    "            \n",
    "    SELECT *\n",
    "    FROM postgres_scan('{postgres_url}','public', 'asset_ownership');\n",
    "    \n",
    "    COPY asset_ownership_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "            \n",
    "    ''')\n",
    "\n",
    "con.close()\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------ GADM 2 Emissions ------------------------------------\n",
    "\n",
    "# import duckdb\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from urllib.parse import quote_plus\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# # Build SQLAlchemy engine for PostgreSQL\n",
    "# user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "# password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "# host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "# port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "# database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "# postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "# parquet_path = \"data/emissions_reduction/gadm_2_emissions.parquet\"\n",
    "\n",
    "# # Use DuckDB to write directly from PostgreSQL to Parquet\n",
    "# con = duckdb.connect()\n",
    "\n",
    "\n",
    "# print('Running query')\n",
    "# con.execute(f'''\n",
    "#     INSTALL postgres;\n",
    "#     LOAD postgres;\n",
    "\n",
    "#     CREATE TABLE gadm_2_emissions_parquet AS\n",
    "#     select extract(year from ge.start_time) as year \n",
    "#         , gb1.gadm_id gadm_1_id\n",
    "#         , gb1.name gadm_1_name\n",
    "#         , gb1.corrected_name gadm_1_corrected_name\n",
    "#         , ge.gadm_id gadm_2_id\n",
    "#         , gb2.name gadm_2_name\n",
    "#         , gb2.corrected_name gadm_2_corrected_name\n",
    "#         , gb2.admin_level\n",
    "#         , ge.iso3_country\n",
    "#         , ca.name as country_name\n",
    "#         , ca.continent\n",
    "#         , ca.eu\n",
    "#         , ca.oecd\n",
    "#         , ca.unfccc_annex\n",
    "#         , ca.developed_un\n",
    "#         , ca.em_finance\n",
    "#         , asch.sector\n",
    "#         , ge.original_inventory_sector subsector\n",
    "#         , ge.gas\n",
    "#         , sum(asset_activity) asset_activity\n",
    "#         , sum(asset_emissions) asset_emissions\n",
    "#         , sum(remainder_activity) remainder_activity\n",
    "#         , sum(remainder_emissions) remainder_emissions\n",
    "#         , sum(asset_emissions) + sum(remainder_emissions) as emissions_quantity\n",
    "\n",
    "#     from postgres_scan('{postgres_url}','public', 'gadm_emissions') ge\n",
    "#     inner join (\n",
    "#         select distinct gadm_id\n",
    "#             , immediate_parent\n",
    "#             , name\n",
    "#             , corrected_name\n",
    "#             , admin_level\n",
    "#         from postgres_scan('{postgres_url}','public', 'gadm_boundaries')\n",
    "#         where admin_level = 2\n",
    "#     ) as gb2\n",
    "#         on ge.gadm_id = gb2.gadm_id\n",
    "#     left join (\n",
    "#         select distinct sector\n",
    "#             , subsector\n",
    "#         from postgres_scan('{postgres_url}','public', 'asset_schema')\n",
    "#     ) asch\n",
    "#         on cast(asch.subsector as varchar) = cast(ge.original_inventory_sector as varchar)\n",
    "#     left join (\n",
    "#         select gadm_id\n",
    "#             , name\n",
    "#             , corrected_name\n",
    "#         from postgres_scan('{postgres_url}','public', 'gadm_boundaries')\n",
    "#         where admin_level = 1\n",
    "#     ) gb1\n",
    "#         on gb1.gadm_id = gb2.immediate_parent\n",
    "#     left join postgres_scan('{postgres_url}','public', 'country_analysis') ca\n",
    "#         on cast(ca.iso3_country as varchar) = cast(ge.iso3_country as varchar)\n",
    "\n",
    "#     where ge.gas = 'co2e_100yr'\n",
    "#         and extract(year from start_time) = 2024\n",
    "#         and ge.original_inventory_sector not in ('forest-land-clearing',\n",
    "#                                                 'forest-land-degradation',\n",
    "#                                                 'forest-land-fires',\n",
    "#                                                 'net-forest-land',\n",
    "#                                                 'net-shrubgrass',\n",
    "#                                                 'net-wetland',\n",
    "#                                                 'removals',\n",
    "#                                                 'shrubgrass-fires',\n",
    "#                                                 'water-reservoirs',\n",
    "#                                                 'wetland-fires')\n",
    "\n",
    "#     group by extract(year from ge.start_time)\n",
    "#         , gb1.gadm_id \n",
    "#         , gb1.name\n",
    "#         , gb1.corrected_name\n",
    "#         , ge.gadm_id \n",
    "#         , gb2.name\n",
    "#         , gb2.corrected_name\n",
    "#         , gb2.admin_level\n",
    "#         , ge.iso3_country\n",
    "#         , ca.name\n",
    "#         , ca.continent\n",
    "#         , ca.eu\n",
    "#         , ca.oecd\n",
    "#         , ca.unfccc_annex\n",
    "#         , ca.developed_un\n",
    "#         , ca.em_finance\n",
    "#         , asch.sector\n",
    "#         , ge.original_inventory_sector\n",
    "#         , ge.gas;\n",
    "\n",
    "#     COPY gadm_2_emissions_parquet TO '{parquet_path}' (FORMAT PARQUET);\n",
    "# ''')\n",
    "# con.close()\n",
    "\n",
    "# print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------------- CITY BATCH -----------------------------------------------------------------\n",
    "\n",
    "# import psycopg2\n",
    "# from urllib.parse import quote_plus\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import os\n",
    "\n",
    "# user = quote_plus(os.getenv(\"CLIMATETRACE_USER\"))\n",
    "# password = quote_plus(os.getenv(\"CLIMATETRACE_PASS\"))\n",
    "# host = os.getenv(\"CLIMATETRACE_HOST\")\n",
    "# port = os.getenv(\"CLIMATETRACE_PORT\")\n",
    "# database = os.getenv(\"CLIMATETRACE_DB\")\n",
    "\n",
    "# conn = psycopg2.connect(\n",
    "#     dbname=database,\n",
    "#     user=user,\n",
    "#     password=password,\n",
    "#     host=host,\n",
    "#     port=port\n",
    "# )\n",
    "\n",
    "# cur = conn.cursor(name='parquet_cursor')  # server-side cursor\n",
    "\n",
    "# cur.execute(\"\"\"\n",
    "#     SELECT extract(year from start_time) AS year,\n",
    "#            ce.city_id,\n",
    "#            cb.name AS city_name,\n",
    "#            cb.corrected_name AS corrected_name,\n",
    "#            ce.iso3_country,\n",
    "#            ca.name AS country_name,\n",
    "#            ca.continent,\n",
    "#            ca.eu,\n",
    "#            ca.oecd,\n",
    "#            ca.unfccc_annex,\n",
    "#            ca.developed_un,\n",
    "#            ca.em_finance,\n",
    "#            asch.sector,\n",
    "#            ce.original_inventory_sector AS subsector,\n",
    "#            SUM(asset_activity) AS asset_activity,\n",
    "#            SUM(asset_emissions) AS asset_emissions,\n",
    "#            SUM(remainder_activity) AS remainder_activity,\n",
    "#            SUM(remainder_emissions) AS remainder_emissions,\n",
    "#            SUM(asset_emissions) + SUM(remainder_emissions) AS emissions_quantity\n",
    "#     FROM city_emissions ce\n",
    "#     LEFT JOIN city_boundaries cb ON cb.city_id = ce.city_id\n",
    "#     LEFT JOIN (\n",
    "#         SELECT DISTINCT sector, subsector FROM asset_schema\n",
    "#     ) asch ON CAST(asch.subsector AS varchar) = CAST(ce.original_inventory_sector AS varchar)\n",
    "#     LEFT JOIN country_analysis ca ON CAST(ca.iso3_country AS varchar) = CAST(ce.iso3_country AS varchar)\n",
    "#     WHERE extract(year FROM ce.start_time) = 2024\n",
    "#       AND ce.gas = 'co2e_100yr'\n",
    "#       AND ce.original_inventory_sector NOT IN (\n",
    "#           'forest-land-clearing', 'forest-land-degradation', 'forest-land-fires',\n",
    "#           'net-forest-land', 'net-shrubgrass', 'net-wetland', 'removals',\n",
    "#           'shrubgrass-fires', 'water-reservoirs', 'wetland-fires'\n",
    "#       )\n",
    "#     GROUP BY extract(year FROM start_time),\n",
    "#              ce.city_id, cb.name, cb.corrected_name,\n",
    "#              ce.iso3_country, ca.name, ca.continent, ca.eu, ca.oecd,\n",
    "#              ca.unfccc_annex, ca.developed_un, ca.em_finance,\n",
    "#              asch.sector, ce.original_inventory_sector\n",
    "# \"\"\")\n",
    "\n",
    "# # Set up Parquet writer\n",
    "# batch_size = 10000\n",
    "# output_file = \"data/emissions_reduction/city_emissions.parquet\"\n",
    "# batch_count = 0\n",
    "# total_rows = 0\n",
    "\n",
    "# print(\"executing city query...\")\n",
    "\n",
    "# # Fetch first batch\n",
    "# rows = cur.fetchmany(batch_size)\n",
    "# if not rows:\n",
    "#     raise Exception(\"No data returned from query.\")\n",
    "\n",
    "# field_names = [desc[0] for desc in cur.description]\n",
    "# first_table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "# writer = pq.ParquetWriter(output_file, first_table.schema)\n",
    "# writer.write_table(first_table)\n",
    "# batch_count += 1\n",
    "# total_rows += len(rows)\n",
    "# print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "# # Process remaining batches\n",
    "# while True:\n",
    "#     rows = cur.fetchmany(batch_size)\n",
    "#     if not rows:\n",
    "#         break\n",
    "\n",
    "#     table = pa.Table.from_pylist([dict(zip(field_names, row)) for row in rows])\n",
    "#     table = table.cast(writer.schema)  # ensure schema matches first batch\n",
    "#     writer.write_table(table)\n",
    "\n",
    "#     batch_count += 1\n",
    "#     total_rows += len(rows)\n",
    "#     print(f\"Processed batch {batch_count} ({len(rows)} rows), total rows: {total_rows}\")\n",
    "\n",
    "# writer.close()\n",
    "# cur.close()\n",
    "# conn.close()\n",
    "# print(\"Export complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01348880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Afghanistan': 'AFG', 'Albania': 'ALB', 'Algeria': 'DZA', 'American Samoa': 'ASM', 'Andorra': 'AND', 'Angola': 'AGO', 'Anguilla': 'AIA', 'Antarctica': 'ATA', 'Antigua and Barbuda': 'ATG', 'Argentina': 'ARG', 'Armenia': 'ARM', 'Aruba': 'ABW', 'Australia': 'AUS', 'Austria': 'AUT', 'Azerbaijan': 'AZE', 'Bahamas': 'BHS', 'Bahrain': 'BHR', 'Bangladesh': 'BGD', 'Barbados': 'BRB', 'Belarus': 'BLR', 'Belgium': 'BEL', 'Belize': 'BLZ', 'Benin': 'BEN', 'Bermuda': 'BMU', 'Bhutan': 'BTN', 'Bolivia': 'BOL', 'Bonaire, Sint Eustatius and Saba': 'BES', 'Bosnia and Herzegovina': 'BIH', 'Botswana': 'BWA', 'Bouvet Island': 'BVT', 'Brazil': 'BRA', 'British Indian Ocean Territory': 'IOT', 'British Virgin Islands': 'VGB', 'Brunei': 'BRN', 'Bulgaria': 'BGR', 'Burkina Faso': 'BFA', 'Burundi': 'BDI', 'Cabo Verde': 'CPV', 'Cambodia': 'KHM', 'Cameroon': 'CMR', 'Canada': 'CAN', 'Cayman Islands': 'CYM', 'Central African Republic': 'CAF', 'Chad': 'TCD', 'Chile': 'CHL', 'China': 'CHN', 'Christmas Island': 'CXR', 'Cocos (Keeling) Islands': 'CCK', 'Colombia': 'COL', 'Comoros': 'COM', 'Congo': 'COG', 'Congo DR': 'COD', 'Cook Islands': 'COK', 'Costa Rica': 'CRI', 'Croatia': 'HRV', 'Cuba': 'CUB', 'Curacao': 'CUW', 'Cyprus': 'CYP', 'Czechia': 'CZE', \"Côte d'Ivoire\": 'CIV', 'Denmark': 'DNK', 'Djibouti': 'DJI', 'Dominica': 'DMA', 'Dominican Republic': 'DOM', 'Ecuador': 'ECU', 'Egypt': 'EGY', 'El Salvador': 'SLV', 'Equatorial Guinea': 'GNQ', 'Eritrea': 'ERI', 'Estonia': 'EST', 'Eswatini': 'SWZ', 'Ethiopia': 'ETH', 'Falkland Islands': 'FLK', 'Faroe Islands': 'FRO', 'Fiji': 'FJI', 'Finland': 'FIN', 'France': 'FRA', 'French Guiana': 'GUF', 'French Polynesia': 'PYF', 'French Southern Territories': 'ATF', 'Gabon': 'GAB', 'Gambia': 'GMB', 'Georgia': 'GEO', 'Germany': 'DEU', 'Ghana': 'GHA', 'Gibraltar': 'GIB', 'Greece': 'GRC', 'Greenland': 'GRL', 'Grenada': 'GRD', 'Guadeloupe': 'GLP', 'Guam': 'GUM', 'Guatemala': 'GTM', 'Guernsey': 'GGY', 'Guinea': 'GIN', 'Guinea-Bissau': 'GNB', 'Guyana': 'GUY', 'Haiti': 'HTI', 'Heard Island and McDonald Islands': 'HMD', 'Holy See': 'VAT', 'Honduras': 'HND', 'Hong Kong': 'HKG', 'Hungary': 'HUN', 'Iceland': 'ISL', 'India': 'IND', 'Indonesia': 'IDN', 'Iran': 'IRN', 'Iraq': 'IRQ', 'Ireland': 'IRL', 'Isle of Man': 'IMN', 'Israel': 'ISR', 'Italy': 'ITA', 'Jamaica': 'JAM', 'Japan': 'JPN', 'Jersey': 'JEY', 'Jordan': 'JOR', 'Kazakhstan': 'KAZ', 'Kenya': 'KEN', 'Kiribati': 'KIR', 'Kosovo': 'XKX', 'Kuwait': 'KWT', 'Kyrgyzstan': 'KGZ', 'Laos': 'LAO', 'Latvia': 'LVA', 'Lebanon': 'LBN', 'Lesotho': 'LSO', 'Liberia': 'LBR', 'Libya': 'LBY', 'Liechtenstein': 'LIE', 'Lithuania': 'LTU', 'Luxembourg': 'LUX', 'Macau': 'MAC', 'Madagascar': 'MDG', 'Malawi': 'MWI', 'Malaysia': 'MYS', 'Maldives': 'MDV', 'Mali': 'MLI', 'Malta': 'MLT', 'Marshall Islands': 'MHL', 'Martinique': 'MTQ', 'Mauritania': 'MRT', 'Mauritius': 'MUS', 'Mayotte': 'MYT', 'Mexico': 'MEX', 'Micronesia (Federated States of)': 'FSM', 'Moldova': 'MDA', 'Monaco': 'MCO', 'Mongolia': 'MNG', 'Montenegro': 'MNE', 'Montserrat': 'MSR', 'Morocco': 'MAR', 'Mozambique': 'MOZ', 'Myanmar': 'MMR', 'Namibia': 'NAM', 'Nauru': 'NRU', 'Nepal': 'NPL', 'Netherlands': 'NLD', 'New Caledonia': 'NCL', 'New Zealand': 'NZL', 'Nicaragua': 'NIC', 'Niger': 'NER', 'Nigeria': 'NGA', 'Niue': 'NIU', 'Norfolk Island': 'NFK', 'North Korea': 'PRK', 'North Macedonia': 'MKD', 'Northern Mariana Islands': 'MNP', 'Norway': 'NOR', 'Oman': 'OMN', 'Pakistan': 'PAK', 'Palau': 'PLW', 'Palestine': 'PSE', 'Panama': 'PAN', 'Papua New Guinea': 'PNG', 'Paraguay': 'PRY', 'Peru': 'PER', 'Philippines': 'PHL', 'Pitcairn': 'PCN', 'Poland': 'POL', 'Portugal': 'PRT', 'Puerto Rico': 'PRI', 'Qatar': 'QAT', 'Romania': 'ROU', 'Russia': 'RUS', 'Rwanda': 'RWA', 'Réunion': 'REU', 'Saint Barthélemy': 'BLM', 'Saint Helena': 'SHN', 'Saint Kitts and Nevis': 'KNA', 'Saint Lucia': 'LCA', 'Saint Martin (French Part)': 'MAF', 'Saint Pierre and Miquelon': 'SPM', 'Saint Vincent and the Grenadines': 'VCT', 'Samoa': 'WSM', 'San Marino': 'SMR', 'Sao Tome and Principe': 'STP', 'Saudi Arabia': 'SAU', 'Senegal': 'SEN', 'Serbia': 'SRB', 'Seychelles': 'SYC', 'Sierra Leone': 'SLE', 'Singapore': 'SGP', 'Sint Maarten (Dutch part)': 'SXM', 'Slovakia': 'SVK', 'Slovenia': 'SVN', 'Solomon Islands': 'SLB', 'Somalia': 'SOM', 'South Africa': 'ZAF', 'South Georgia and the South Sandwich Islands': 'SGS', 'South Korea': 'KOR', 'South Sudan': 'SSD', 'Spain': 'ESP', 'Sri Lanka': 'LKA', 'Sudan': 'SDN', 'Suriname': 'SUR', 'Svalbard and Jan Mayen Islands': 'SJM', 'Sweden': 'SWE', 'Switzerland': 'CHE', 'Syria': 'SYR', 'Taiwan': 'TWN', 'Tajikistan': 'TJK', 'Tanzania': 'TZA', 'Thailand': 'THA', 'Timor-Leste': 'TLS', 'Togo': 'TGO', 'Tokelau': 'TKL', 'Tonga': 'TON', 'Trinidad and Tobago': 'TTO', 'Tunisia': 'TUN', 'Turkey': 'TUR', 'Turkmenistan': 'TKM', 'Turks and Caicos Islands': 'TCA', 'Tuvalu': 'TUV', 'Uganda': 'UGA', 'Ukraine': 'UKR', 'United Arab Emirates': 'ARE', 'United Kingdom': 'GBR', 'United States Minor Outlying Islands': 'UMI', 'United States Virgin Islands': 'VIR', 'United States of America': 'USA', 'Unlisted Country': 'ZNC', 'Uruguay': 'URY', 'Uzbekistan': 'UZB', 'Vanuatu': 'VUT', 'Venezuela': 'VEN', 'Vietnam': 'VNM', 'Wallis and Futuna Islands': 'WLF', 'Western Sahara': 'ESH', 'Yemen': 'YEM', 'Zambia': 'ZMB', 'Zimbabwe': 'ZWE', 'Åland Islands': 'ALA'}\n"
     ]
    }
   ],
   "source": [
    "# import duckdb\n",
    "# from config import CONFIG\n",
    "\n",
    "# con = duckdb.connect()\n",
    "# # asset_annual_path = 'data/asset_annual_emissions/chunk_*.parquet'\n",
    "# asset_path = CONFIG['asset_path']\n",
    "# country_subsector_stats_path = CONFIG['country_subsector_stats_path']\n",
    "# country_subsector_totals_path = CONFIG['country_subsector_totals_path']\n",
    "# region_options = CONFIG['region_options']\n",
    "# gadm_0_path = CONFIG['gadm_0_path']\n",
    "\n",
    "# country_rows = con.execute(\n",
    "#             f\"SELECT DISTINCT country_name, iso3_country FROM '{gadm_0_path}' WHERE country_name IS NOT NULL order by country_name\"\n",
    "#         ).fetchall()\n",
    "# country_map = {row[0]: row[1] for row in country_rows}\n",
    "# unique_countries = list(country_map.keys())\n",
    "\n",
    "# print(country_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0707b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Replace with your CSV file path\n",
    "# csv_file = \"/Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/static/ct_percentile_40sectors_moer_stat_industrial_20250824.csv\"\n",
    "# parquet_file = \"/Users/anthonyrusso/Dev/emissions-reduction-pathways-dashboard/data/static/ct_percentile_40sectors_moer_stat_industrial_20250824.parquet\"\n",
    "\n",
    "# # Read CSV\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Save as Parquet\n",
    "# df.to_parquet(parquet_file, index=False)\n",
    "\n",
    "# print(f\"Converted {csv_file} to {parquet_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
